{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d8b141-b3fd-47bb-9b6d-d65067620d97",
   "metadata": {},
   "source": [
    "<a   href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/demo_VideoDataFrame_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cba83-73af-4139-a22e-accb576fd93d",
   "metadata": {},
   "source": [
    "# Demo VideoDataFrame Class \n",
    "This demo uses the Algonauts dataset.\n",
    "    \n",
    "TABLE OF CODE CONTENTS:\n",
    "1. Minimal demo without image transforms\n",
    "2. Minimal demo without sparse temporal sampling for single continuous frame clips, without image transforms\n",
    "3. Demo with image transforms\n",
    "4. Demo with image transforms and dataloader\n",
    "\n",
    "For more details about the VideoDataFrame Class, see the [VideoDataset Repo](https://video-dataset-loading-pytorch.readthedocs.io/en/latest/VideoDataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fc245-da1a-45da-ae03-4e422e0e199d",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a50609-1ebf-4460-8833-67b5947a9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Buzznauts\n",
    "!pip install duecredit --quiet\n",
    "!git clone https://github.com/eduardojdiniz/Buzznauts --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1f26e-b3d6-44b6-9d54-baac0a07ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae14828-28a1-44b5-9dee-022726693a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Buzznauts data functions\n",
    "from Buzznauts.Buzznauts.data.utils import plot_video\n",
    "from Buzznauts.Buzznauts.data.videodataframe import VideoFrameDataset, ImglistToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acaa35b-9ab9-4f22-9a13-8457a0f6db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch\n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289baca9-2d32-44cd-a5ca-d43e0cd2d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set videos and annotation file path\n",
    "import os.path as op\n",
    "stimuli = \"/content/drive/MyDrive/Buzznauts/data/stimuli\"\n",
    "videos_root = op.join(stimuli, \"frames\")\n",
    "annotations_file = op.join(videos_root, \"annotations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519f2ef-4401-49ca-b461-30a3f92b0105",
   "metadata": {},
   "source": [
    "### Demo 1 - Sampled Frames, without Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b19d6-06b1-40d4-a290-e5bf8944be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFrameDataset(\n",
    "    root_path=videos_root,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=3,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=None,\n",
    "    random_shift=True,\n",
    "    test_mode=False)\n",
    "\n",
    "sample = dataset[0]\n",
    "frames = sample[0]  # list of PIL images\n",
    "label = sample[1]   # integer label\n",
    "\n",
    "plot_video(rows=1, cols=3, frame_list=frames, plot_width=15., plot_height=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d1a9f-b006-4f94-8d06-9451d6f5c5bf",
   "metadata": {},
   "source": [
    "### Demo 2 - Single Continuous Frame Clip instead of Sampled Frames, without Image Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbb416-25a2-450e-9a16-121780bdd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFrameDataset(\n",
    "        root_path=videos_root,\n",
    "        annotationfile_path=annotation_file,\n",
    "        num_segments=1,\n",
    "        frames_per_segment=9,\n",
    "        imagefile_template='img_{:05d}.jpg',\n",
    "        transform=None,\n",
    "        random_shift=True,\n",
    "        test_mode=False)\n",
    "\n",
    "sample = dataset[5]\n",
    "frames = sample[0]  # list of PIL images\n",
    "label = sample[1]  # integer label\n",
    "\n",
    "plot_video(rows=3, cols=3, frame_list=frames, plot_width=10., plot_height=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692a013-ba1f-4d83-948f-6efe0087acc9",
   "metadata": {},
   "source": [
    "### Demo 3 - Sampled Frames, with Image Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9305e-caef-4016-8bd7-79a7e4f231aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(video_tensor):\n",
    "    \"\"\"Undoes mean/standard deviation normalization, zero to one scaling, and channel rearrangement for a batch of images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    video_tensor : tensor.FloatTensor \n",
    "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    video_array : numpy.ndarray[float]\n",
    "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) numpy array of floats\n",
    "    \"\"\"\n",
    "    inverse_normalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225])\n",
    "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6f2f9-abac-4ec0-a8f3-a9c629b38194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of torchvision 0.8.0, torchvision transforms support batches of images\n",
    "# of size (BATCH x CHANNELS x HEIGHT x WIDTH) and apply deterministic or random\n",
    "# transformations on the batch identically on all images of the batch. Any torchvision\n",
    "# transform for image augmentation can thus also be used  for video augmentation.\n",
    "preprocess = transforms.Compose([\n",
    "    ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    transforms.Resize(299),  # image batch, resize smaller edge to 299\n",
    "    transforms.CenterCrop(299),  # image batch, center crop to square 299x299\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path=videos_root,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=5,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=preprocess,\n",
    "    random_shift=True,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "sample = dataset[2]\n",
    "frame_tensor = sample[0]  # tensor of shape (NUM_SEGMENTS*FRAMES_PER_SEGMENT) x CHANNELS x HEIGHT x WIDTH\n",
    "label = sample[1]  # integer label\n",
    "\n",
    "print('Video Tensor Size:', frame_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8fd2c-dbd2-4ac3-8174-26028cc8550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array = denormalize(frame_tensor)\n",
    "plot_video(rows=1, cols=5, frame_list=frame_array, plot_width=15., plot_height=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1372bb-6453-4a35-a928-ae12eba6d63e",
   "metadata": {},
   "source": [
    "### Demo 4 - Sampled Frames Dataloader, with Image Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d7566-6595-4596-a467-e762ede9b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for video_batch, labels in dataloader:\n",
    "        \"\"\"\n",
    "        Insert Training Code Here\n",
    "        \"\"\"\n",
    "        print(labels)\n",
    "        print(\"\\nVideo Batch Tensor Size:\", video_batch.size())\n",
    "        print(\"Batch Labels Size:\", labels.size())\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Buzznauts)",
   "language": "python",
   "name": "buzznauts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
