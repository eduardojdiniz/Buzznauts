{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wandb_VAE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Pytorch (Buzznauts)",
      "language": "python",
      "name": "buzznauts"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f58bad84cb5a4536b012893f48f10584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3ba11a50569b44b98b97dd23ecabf663",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b91371c3dfe840358558777e074c02c5",
              "IPY_MODEL_9d076b5ffc8547a8842c6a2af32fb4d8",
              "IPY_MODEL_cda0214a50f84744853639c57445c3dc"
            ]
          }
        },
        "3ba11a50569b44b98b97dd23ecabf663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b91371c3dfe840358558777e074c02c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6122fd1fbdba4fa08b2d74aa912a5810",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epochs:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86d5cd719c874409a8bdba9f55638bd7"
          }
        },
        "9d076b5ffc8547a8842c6a2af32fb4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_21688de5d15c462585c954a1ec9baa4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bac85a637d2743b3933f571d97f2be49"
          }
        },
        "cda0214a50f84744853639c57445c3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3252b13ba35045ec9132ef3a279c71bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/10 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_737cb4862f7c47beb50d888602664217"
          }
        },
        "6122fd1fbdba4fa08b2d74aa912a5810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86d5cd719c874409a8bdba9f55638bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21688de5d15c462585c954a1ec9baa4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bac85a637d2743b3933f571d97f2be49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3252b13ba35045ec9132ef3a279c71bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "737cb4862f7c47beb50d888602664217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96d3f1e85f9f466ca5da931cc559c008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c33dca608c143628bf8dffb3c05d37d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b5046a4113449f6821a75af64c4455b",
              "IPY_MODEL_f0286f406cff4b9c8e1a720c05cc323c",
              "IPY_MODEL_a7f25b09e790409d805de8571548eb8e"
            ]
          }
        },
        "0c33dca608c143628bf8dffb3c05d37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b5046a4113449f6821a75af64c4455b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_505f1a38623542e0a75874578b950d4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Batches:   1%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c7882ca81704adeb89d6ad483ca2133"
          }
        },
        "f0286f406cff4b9c8e1a720c05cc323c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70d0980ee5ab49b8917f5fdf138e01ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1653,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73e4ea0b4539410c92312b42973ab769"
          }
        },
        "a7f25b09e790409d805de8571548eb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16afc0dd3e33410193abdca5bc04ab62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/1653 [13:58&lt;15:28:40, 34.12s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e723a134509640ccbd18197395037988"
          }
        },
        "505f1a38623542e0a75874578b950d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c7882ca81704adeb89d6ad483ca2133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70d0980ee5ab49b8917f5fdf138e01ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73e4ea0b4539410c92312b42973ab769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16afc0dd3e33410193abdca5bc04ab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e723a134509640ccbd18197395037988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhH5pMDk50x7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/wandb_VAE_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-AIidsa50x8"
      },
      "source": [
        "# VAE Model: Predicting fMRI responses from Algonauts2021 dataset\n",
        "\n",
        "**Goal:** Prepare submission for Algonauts 2021 challenge using the VAE model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXd2dZAH50x9"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4ImCwz550x-"
      },
      "source": [
        "# Install dependencies and Download Buzznauts\n",
        "%%capture\n",
        "\n",
        "!pip install duecredit --quiet\n",
        "!pip install torchinfo --quiet\n",
        "!pip install nilearn --quiet\n",
        "!pip install decord --quiet\n",
        "!pip install git+https://github.com/eduardojdiniz/Buzznauts --quiet\n",
        "!pip install wandb --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVyFQX3M50x_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db84752f-3cb2-44db-d874-3c055dd88d9b"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFfQajf950yA"
      },
      "source": [
        "# Set videos and annotation file path\n",
        "import os\n",
        "import os.path as op\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "\n",
        "drive_root = \"/content/drive/MyDrive/Buzznauts\"\n",
        "\n",
        "# Data paths\n",
        "fmri_dir = op.join(drive_root, \"data\", \"fmri\")\n",
        "stimuli = op.join(drive_root, \"data\", \"stimuli\") \n",
        "videos_dir = op.join(stimuli, \"videos\")\n",
        "frames_dir = op.join(stimuli, \"frames\")\n",
        "annotation_file = op.join(frames_dir, 'annotations.txt')\n",
        "pretrained_dir = op.join(drive_root, \"data\", \"pretrained\")\n",
        "pretrained_vaegan = op.join(pretrained_dir, \"vaegan_enc_weights.pickle\")\n",
        "\n",
        "# Visualizations path\n",
        "viz_dir = \"/content/visualizations\"\n",
        "viz_vae_dir = op.join(viz_dir, \"vae\")\n",
        "\n",
        "# Model path\n",
        "models_dir = \"/content/models\"\n",
        "model_vae_dir = op.join(models_dir, \"vae\")\n",
        "\n",
        "# Results paths\n",
        "results_dir = \"/content/results/vae\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkxTLlz50yB"
      },
      "source": [
        "# Import interactive tools\n",
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc12C_gr50yC"
      },
      "source": [
        "# Import pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmBOsJxD50yD"
      },
      "source": [
        "from Buzznauts.utils import set_seed, set_device, seed_worker, set_generator\n",
        "from Buzznauts.data.utils import plot_video_frames\n",
        "from Buzznauts.data.videodataframe import VideoFrameDataset, ImglistToTensor, FrameDataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9suFvfF50yE"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3e7P69y50yF"
      },
      "source": [
        "class ConvVarAutoEncoder(nn.Module):\n",
        "    def __init__(self, K, data_shape=(3, 128, 128), num_filters=[192, 256, 384, 512, 768], filter_size=3):\n",
        "        super(ConvVarAutoEncoder, self).__init__()\n",
        "        ## 5 Conv Layers\n",
        "        filter_reduction = 5 * (filter_size // 2)\n",
        "\n",
        "        self.shape_after_conv = calc_output_size(data_shape, filter_size, num_filters)\n",
        "\n",
        "        self.flat_shape = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
        "\n",
        "        # Double for each additional layer of Conv\n",
        "        flat_size_after_conv = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
        "\n",
        "        # ENCODER\n",
        "        self.q_bias = BiasLayer(data_shape)\n",
        "        self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters[0], filter_size)\n",
        "        self.q_conv_2 = nn.Conv2d(num_filters[0], num_filters[1], filter_size)\n",
        "        self.q_conv_3 = nn.Conv2d(num_filters[1], num_filters[2], filter_size)\n",
        "        self.q_conv_4 = nn.Conv2d(num_filters[2], num_filters[3], filter_size)\n",
        "        self.q_conv_5 = nn.Conv2d(num_filters[3], num_filters[4], filter_size)\n",
        "        self.q_flatten = nn.Flatten()\n",
        "        self.q_fc_phi = nn.Linear(self.flat_shape, K+1)\n",
        "\n",
        "        # DECODER\n",
        "        self.p_fc_upsample = nn.Linear(K, self.flat_shape)\n",
        "        self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
        "        self.p_deconv_1 = nn.ConvTranspose2d(num_filters[4], num_filters[3], filter_size)\n",
        "        self.p_deconv_2 = nn.ConvTranspose2d(num_filters[3], num_filters[2], filter_size)\n",
        "        self.p_deconv_3 = nn.ConvTranspose2d(num_filters[2], num_filters[1], filter_size)\n",
        "        self.p_deconv_4 = nn.ConvTranspose2d(num_filters[1], num_filters[0], filter_size)\n",
        "        self.p_deconv_5 = nn.ConvTranspose2d(num_filters[0], data_shape[0], filter_size)\n",
        "\n",
        "        self.p_bias = BiasLayer(data_shape)\n",
        "\n",
        "        # Define a special extra parameter to learn scalar sig_x for all pixels\n",
        "        self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "\n",
        "    def infer(self, x):\n",
        "        \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
        "        rsample to get z\n",
        "        \"\"\"\n",
        "        s = self.q_bias(x)\n",
        "        s = F.elu(self.q_conv_1(s))\n",
        "        s = F.elu(self.q_conv_2(s))\n",
        "        s = F.elu(self.q_conv_3(s))\n",
        "        s = F.elu(self.q_conv_4(s))\n",
        "        s = F.elu(self.q_conv_5(s))\n",
        "        flat_s = s.view(s.size()[0], -1)\n",
        "        phi = self.q_fc_phi(flat_s)\n",
        "        return phi\n",
        "\n",
        "\n",
        "    def generate(self, zs):\n",
        "        \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
        "        \"\"\"\n",
        "        # Note that for the purposes of passing through the generator, we need\n",
        "        # to reshape zs to be size [b*n,k]\n",
        "        b, n, k = zs.size()\n",
        "        s = zs.view(b*n, -1)\n",
        "        s = F.elu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
        "        s = F.elu(self.p_deconv_1(s))\n",
        "        s = F.elu(self.p_deconv_2(s))\n",
        "        s = F.elu(self.p_deconv_3(s))\n",
        "        s = F.elu(self.p_deconv_4(s))\n",
        "        s = self.p_deconv_5(s)\n",
        "        s = self.p_bias(s)\n",
        "        mu_xs = s.view(b, n, -1)\n",
        "        return mu_xs\n",
        "\n",
        "    \n",
        "    def decode(self, zs):\n",
        "        # Included for compatability with conv-AE code\n",
        "        return self.generate(zs.unsqueeze(0))\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # VAE.forward() is not used for training, but we'll treat it like a\n",
        "        # classic autoencoder by taking a single sample of z ~ q\n",
        "        phi = self.infer(x)\n",
        "        zs = rsample(phi, 1)\n",
        "        return self.generate(zs).view(x.size())\n",
        "    \n",
        "\n",
        "    def elbo(self, x, n=1, epsilon = 0.1):\n",
        "        \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
        "        samples of z\n",
        "        \"\"\"\n",
        "        phi = self.infer(x)\n",
        "        zs = rsample(phi, n)\n",
        "        mu_xs = self.generate(zs)\n",
        "        return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi, epsilon), mu_xs.data\n",
        "\n",
        "    \n",
        "    def load_my_state_dict(self, state_dict):\n",
        "        curr_state=self.state_dict()\n",
        "\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in curr_state:\n",
        "                continue\n",
        "            if isinstance(param, torch.Tensor):\n",
        "                param = param.data\n",
        "            curr_state[name].copy_(param)\n",
        "    \n",
        "    \n",
        "class BiasLayer(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(BiasLayer, self).__init__()\n",
        "        init_bias = torch.zeros(shape)\n",
        "        self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.bias\n",
        "    \n",
        "    \n",
        "def calc_output_size(input_size, kernel_size, kchannels, padding=0, stride=1):\n",
        "    output_size = input_size\n",
        "    for kc in kchannels:\n",
        "        output_height = (output_size[1] + padding + padding - kernel_size) / (stride) + 1\n",
        "        output_width = (output_size[2] + padding + padding - kernel_size) / (stride) + 1\n",
        "\n",
        "        output_size = [kc, int(output_height), int(output_width)]\n",
        "\n",
        "    return tuple(output_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFDK4_BW50yG"
      },
      "source": [
        "### ELBO loss helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC7JE7lK50yG"
      },
      "source": [
        "def kl_q_p(zs, phi, epsilon=0.1):\n",
        "    \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
        "    phi must be size [b,k+1]\n",
        "\n",
        "    This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
        "    just -1/2*(zs**2)\n",
        "    \"\"\"\n",
        "    b, n, k = zs.size()\n",
        "    mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
        "    log_p = -0.5*(zs**2)\n",
        "   \n",
        "    log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / (log_sig_q.exp().view(b,1,1)**2 + epsilon) - log_sig_q.view(b,1,-1)\n",
        "    # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
        "    return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def log_p_x(x, mu_xs, sig_x):\n",
        "    \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
        "    pixel-wise log Gaussian probability\n",
        "\n",
        "    Sum over pixel dimensions, but mean over batch and samples.\n",
        "    \"\"\"\n",
        "    b, n = mu_xs.size()[:2]\n",
        "    # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
        "    # implicitly expanded when combined with mu_xs\n",
        "    x = x.reshape(b, 1, -1)\n",
        "    _, _, p = x.size()\n",
        "    squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
        "\n",
        "    # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
        "    # Expected value requires mean over [n]. Handling different size batches\n",
        "    # requires mean over [b].\n",
        "    return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def rsample(phi, n_samples):\n",
        "    \"\"\"Sample z ~ q(z;phi)\n",
        "    Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
        "    entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
        "    standard deviation\n",
        "    \"\"\"\n",
        "    b, kplus1 = phi.size()\n",
        "    k = kplus1-1\n",
        "    mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
        "    eps = torch.randn(b, n_samples, k, device=phi.device)\n",
        "    return eps*sig.view(b,1,1) + mu.view(b,1,k)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcyCsYbF50yH"
      },
      "source": [
        "### Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09EB88cm50yH"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLmz8wNr50yI"
      },
      "source": [
        "def load_vaegan_weights(model, pretrained_path):\n",
        "    # load pretrained weights\n",
        "    pretrained_fn = open(pretrained_path,'rb')\n",
        "    pretrained = pickle.load(pretrained_fn)\n",
        "\n",
        "    # have a look what's in the pretrained file\n",
        "    old_keynames=[]\n",
        "    for key, value in pretrained.items():\n",
        "        old_keynames.append(key) \n",
        "\n",
        "    # get the keynames of our model\n",
        "    curr_state=model.state_dict()\n",
        "    new_keynames=[]\n",
        "    for key, value in curr_state.items():\n",
        "        if key.startswith('q_conv'):\n",
        "            new_keynames.append(key)\n",
        "\n",
        "    # change the names of the pretrained model to match our model\n",
        "    for i in range(len(old_keynames)):\n",
        "        pretrained[new_keynames[i]] = pretrained[old_keynames[i]]\n",
        "        del pretrained[old_keynames[i]]\n",
        "\n",
        "    # change size & make the weights a torch\n",
        "    # In TF, Conv2d filter shape is [filter_height, filter_width, in_channels, out_channels],\n",
        "    # while in Pytorch is (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
        "    # So we need to permute [3,2,0,1]\n",
        "    for key, value in pretrained.items():\n",
        "        if len(value.shape)==4:\n",
        "            new_val=torch.tensor(value)\n",
        "            new_val=new_val.permute(3,2,0,1)\n",
        "        else: \n",
        "            new_val=torch.tensor(value)\n",
        "    \n",
        "        pretrained[key] = new_val\n",
        "    \n",
        "    return pretrained"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZZ27EJj50yL"
      },
      "source": [
        "def reset_weights(model):\n",
        "    \"\"\"Try resetting model weights to avoid weight leakage.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "    \"\"\"\n",
        "    for layer in model.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NOp2dNZ50yM"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df46NodF50yM"
      },
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30vHYppF50yM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d24dd7-530a-4d9b-e016-b0e442884275"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "# Set seed to the random generators to ensure reproducibility\n",
        "seed = set_seed()\n",
        "\n",
        "# Set computational device (cuda if GPU is available, else cpu)\n",
        "device = set_device()\n",
        "      \n",
        "# Number of folds for cross-validation \n",
        "k_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 10 \n",
        "\n",
        "# Batch size\n",
        "batch_size = 4 \n",
        "\n",
        "# Size of the VAE's latent space  \n",
        "K_VAE = 128 \n",
        "\n",
        "#---------------\n",
        "# Create Dataset\n",
        "#---------------\n",
        "\n",
        "# Number of splits in each video\n",
        "num_segments = 5\n",
        "\n",
        "# Number of frames per split\n",
        "frames_per_segment = 6\n",
        "\n",
        "# Total number of training frames\n",
        "total_frames = num_segments * frames_per_segment\n",
        "\n",
        "# Frame size\n",
        "frame_size = 32 \n",
        "width = frame_size\n",
        "height = frame_size\n",
        "\n",
        "# Num of channels\n",
        "num_channels = 3\n",
        "\n",
        "# Data shape\n",
        "data_shape = (num_channels, frame_size, frame_size)\n",
        "\n",
        "# Input size\n",
        "input_size = (batch_size, num_channels, frame_size, frame_size)\n",
        "\n",
        "# Tensorize convert PIL images to tensors and resize each frame to frame_size\n",
        "tensorize = transforms.Compose([\n",
        "    ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
        "    transforms.Resize(frame_size), # image batch, resize smaller edge to 128\n",
        "])\n",
        "\n",
        "# Preprocess center crop to 100x128, normalize and apply random affine\n",
        "# and horizontal flips to each frame\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.CenterCrop((frame_size, frame_size)), # image batch, center crop to square 100x128\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.78125, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "])\n",
        "\n",
        "# Videoframe dataset: each sample is of size (FRAMES X CHANNELS X HEIGHT X WIDTH)\n",
        "videoframe_dataset = VideoFrameDataset(\n",
        "    root_path=frames_dir,\n",
        "    annotationfile_path=annotation_file,\n",
        "    num_segments=num_segments,\n",
        "    frames_per_segment=frames_per_segment,\n",
        "    imagefile_template='img_{:05d}.jpg',\n",
        "    transform=tensorize,\n",
        "    random_shift=False,\n",
        "    test_mode=False\n",
        ")\n",
        "\n",
        "# Frame dataset: each sample is of size (CHANNELS X HEIGHT X WIDTH)\n",
        "frame_dataset = FrameDataset(\n",
        "    videoframedataset=videoframe_dataset,\n",
        "    transform=preprocess\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 724233587 has been set.\n",
            "GPU is not enabled.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNZlwQ_m50yN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "003d4383-b7e3-4e47-e17f-a1c4a0f9cc1a"
      },
      "source": [
        "# Model summary\n",
        "from torchinfo import summary\n",
        "\n",
        "convVAE = ConvVarAutoEncoder(data_shape=data_shape, K=K_VAE)\n",
        "wandb.init()\n",
        "wandb.watch(convVAE)\n",
        "summary(convVAE, input_size=input_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">swift-salad-35</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rwoodry/uncategorized\" target=\"_blank\">https://wandb.ai/rwoodry/uncategorized</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/rwoodry/uncategorized/runs/2okq42iv\" target=\"_blank\">https://wandb.ai/rwoodry/uncategorized/runs/2okq42iv</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210819_172716-2okq42iv</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ConvVarAutoEncoder                       --                        --\n",
              "├─BiasLayer: 1-1                         [4, 3, 32, 32]            3,072\n",
              "├─Conv2d: 1-2                            [4, 192, 30, 30]          5,376\n",
              "├─Conv2d: 1-3                            [4, 256, 28, 28]          442,624\n",
              "├─Conv2d: 1-4                            [4, 384, 26, 26]          885,120\n",
              "├─Conv2d: 1-5                            [4, 512, 24, 24]          1,769,984\n",
              "├─Conv2d: 1-6                            [4, 768, 22, 22]          3,539,712\n",
              "├─Linear: 1-7                            [4, 129]                  47,950,977\n",
              "├─Linear: 1-8                            [4, 371712]               47,950,848\n",
              "├─ConvTranspose2d: 1-9                   [4, 512, 24, 24]          3,539,456\n",
              "├─ConvTranspose2d: 1-10                  [4, 384, 26, 26]          1,769,856\n",
              "├─ConvTranspose2d: 1-11                  [4, 256, 28, 28]          884,992\n",
              "├─ConvTranspose2d: 1-12                  [4, 192, 30, 30]          442,560\n",
              "├─ConvTranspose2d: 1-13                  [4, 3, 32, 32]            5,187\n",
              "├─BiasLayer: 1-14                        [4, 3, 32, 32]            3,072\n",
              "==========================================================================================\n",
              "Total params: 109,192,836\n",
              "Trainable params: 109,192,836\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 32.45\n",
              "==========================================================================================\n",
              "Input size (MB): 0.05\n",
              "Forward/backward pass size (MB): 83.48\n",
              "Params size (MB): 436.77\n",
              "Estimated Total Size (MB): 520.30\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtTsrAJ50yN"
      },
      "source": [
        "#### Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgf6U5Lg50yN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "f58bad84cb5a4536b012893f48f10584",
            "3ba11a50569b44b98b97dd23ecabf663",
            "b91371c3dfe840358558777e074c02c5",
            "9d076b5ffc8547a8842c6a2af32fb4d8",
            "cda0214a50f84744853639c57445c3dc",
            "6122fd1fbdba4fa08b2d74aa912a5810",
            "86d5cd719c874409a8bdba9f55638bd7",
            "21688de5d15c462585c954a1ec9baa4e",
            "bac85a637d2743b3933f571d97f2be49",
            "3252b13ba35045ec9132ef3a279c71bb",
            "737cb4862f7c47beb50d888602664217",
            "96d3f1e85f9f466ca5da931cc559c008",
            "0c33dca608c143628bf8dffb3c05d37d",
            "6b5046a4113449f6821a75af64c4455b",
            "f0286f406cff4b9c8e1a720c05cc323c",
            "a7f25b09e790409d805de8571548eb8e",
            "505f1a38623542e0a75874578b950d4e",
            "3c7882ca81704adeb89d6ad483ca2133",
            "70d0980ee5ab49b8917f5fdf138e01ba",
            "73e4ea0b4539410c92312b42973ab769",
            "16afc0dd3e33410193abdca5bc04ab62",
            "e723a134509640ccbd18197395037988"
          ]
        },
        "outputId": "479c0ace-bbe1-43de-e914-c13fb7b99261"
      },
      "source": [
        " torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Save loss values during training for each fold\n",
        "loss_train = {f'Fold_{i}': [] for i in range(1, k_folds+1)}\n",
        "# Save loss during validation for each fold\n",
        "loss_val = {f'Fold_{i}': [] for i in range(1, k_folds+1)}\n",
        "# Save overall loss during validation for each fold\n",
        "loss_val_overall = {f'Fold_{i}': None for i in range(1, k_folds+1)}\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(frame_dataset)):\n",
        "    print(f'FOLD {fold+1}')\n",
        "    print('-------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of idx, no replacement\n",
        "    train_subsampler = SubsetRandomSampler(train_idx)\n",
        "    val_subsampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    train_loader = DataLoader(\n",
        "        dataset=frame_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_subsampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=set_generator())\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        dataset=frame_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=val_subsampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=set_generator())\n",
        "    \n",
        "    # Instantiate network\n",
        "    convVAE = ConvVarAutoEncoder(data_shape=data_shape, K=K_VAE)\n",
        "    convVAE.apply(reset_weights)\n",
        "\n",
        "    # Load Pretrained weights\n",
        "    # pretrained = load_vaegan_weights(convVAE, pretrained_vaegan)\n",
        "    # convVAE.load_my_state_dict(pretrained)\n",
        "    \n",
        "    # # Freezing layers\n",
        "    # freeze_idx = [2, 3, 4, 5]\n",
        "    # for idx, param in enumerate(convVAE.parameters()): \n",
        "    #     if idx in freeze_idx: param.requires_grad = False\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, convVAE.parameters()),\n",
        "                                 lr=3e-4, weight_decay=0)\n",
        "    \n",
        "    convVAE.to(device)\n",
        "    convVAE.train()\n",
        "    \n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in trange(num_epochs, desc='Epochs'):\n",
        "        \n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "        \n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, (frame, label) in enumerate(tqdm(train_loader, \n",
        "                                                total=len(train_loader) // batch_size,\n",
        "                                                desc='Batches', leave=False)):\n",
        "            frame = frame.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute loss\n",
        "            loss, muxs = convVAE.elbo(frame)\n",
        "            loss = -loss\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Saving loss\n",
        "            loss_train[f'Fold_{fold+1}'].append(-loss.item())\n",
        "\n",
        "            # Print statistics\n",
        "            current_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, current_loss / 100))\n",
        "                current_loss = 0.0\n",
        "            \n",
        "            example_frames = [wandb.Image(f, caption=\"Input {idx}\".format(idx=idx)) for idx, f in enumerate(frame)]\n",
        "            example_gens = [wandb.Image(g.reshape(3, 32, 32), caption=\"Generated {idx}\".format(idx=idx)) for idx, g in enumerate(muxs)]\n",
        "\n",
        "            wandb.log({\"Loss: \" : loss.item(),\n",
        "                       \"Input: \": example_frames,\n",
        "                       \"Generated: \": example_gens})\n",
        "        \n",
        "    # Evaluation for this fold\n",
        "    convVAE.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the DataLoader for validation data\n",
        "        for i, (frame, label) in enumerate(tqdm(val_loader, \n",
        "                                                total=len(val_loader) // batch_size,\n",
        "                                                desc='Batches', leave=False)):\n",
        "            # Compute loss\n",
        "            loss = -convVAE.elbo(frame)\n",
        "\n",
        "            # Saving loss\n",
        "            loss_val[f'Fold_{fold+1}'].append(-loss.item())\n",
        "\n",
        "        # Print overall fold loss \n",
        "        loss_val_overall[f'Fold_{fold+1}'] = sum(loss_val[f'Fold_{fold+1}'])\n",
        "        print('Total loss for fold %d: %d %%' % (fold, results[f'Fold_{fold+1}']))\n",
        "        print('--------------------------------')\n",
        "    \n",
        "# Print fold results\n",
        "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "print('----------------------------------------------------')\n",
        "overall_sum = 0.0\n",
        "for key, value in loss_val_overall.items():\n",
        "    print(f'Fold {key+1}: {value} %')\n",
        "    overall_sum += value\n",
        "print(f'Average: {overall_sum/len(loss_val_overall.items())} %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 1\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f58bad84cb5a4536b012893f48f10584",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96d3f1e85f9f466ca5da931cc559c008",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Batches:   0%|          | 0/1653 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "rGMDd4DP7pFh",
        "outputId": "00de172d-87c3-4572-d223-61d0e6bfdc48"
      },
      "source": [
        "elbo_vals"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-c0b068a6549c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melbo_vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'elbo_vals' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj-mc6ybF8ko",
        "outputId": "215178d8-504e-4801-a96f-c91c29c38e46"
      },
      "source": [
        "3*32*32\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5qARmSFGWdw",
        "outputId": "fb94dd3f-95b9-4ee5-9f3e-45a798cdc86e"
      },
      "source": [
        "for f in frame: print(f"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCAcQamFUYYL",
        "outputId": "6c18d786-d3cd-4a21-e72a-529958b75bda"
      },
      "source": [
        "mu_xs[0].reshape(3, 32, 32)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRNFN1zJYTaO",
        "outputId": "4ecb6c0a-cc51-4050-8dce-48cb788dfca8"
      },
      "source": [
        "mu_xs.data"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[nan, nan, nan,  ..., nan, nan, nan]],\n",
              "\n",
              "        [[nan, nan, nan,  ..., nan, nan, nan]],\n",
              "\n",
              "        [[nan, nan, nan,  ..., nan, nan, nan]],\n",
              "\n",
              "        [[nan, nan, nan,  ..., nan, nan, nan]]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvbRh1gKcSZI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}