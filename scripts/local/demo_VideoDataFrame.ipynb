{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297cba83-73af-4139-a22e-accb576fd93d",
   "metadata": {},
   "source": [
    "# Demo VideoDataFrame Class \n",
    "This demo uses the Algonauts dataset.\n",
    "    \n",
    "TABLE OF CODE CONTENTS:\n",
    "1. Minimal demo without image transforms\n",
    "2. Minimal demo without sparse temporal sampling for single continuous frame clips, without image transforms\n",
    "3. Demo with image transforms\n",
    "4. Demo with image transforms and dataloader\n",
    "5. Demo with image transforms, dataloader and K-fold Cross-Validation\n",
    "\n",
    "For more details about the VideoDataFrame Class, see the [VideoDataset Repo](https://video-dataset-loading-pytorch.readthedocs.io/en/latest/VideoDataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fc245-da1a-45da-ae03-4e422e0e199d",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1ba98-1bcc-467f-b2e3-4ff4926420ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ad55a-85a4-429e-b143-473c4b63086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from pathlib import Path\n",
    "import Buzznauts as buzz\n",
    "buzz_root = Path(buzz.__path__[0]).parent.absolute()\n",
    "\n",
    "# Data paths\n",
    "fmri_dir = op.join(buzz_root, \"data\", \"fmri\")\n",
    "stimuli = op.join(buzz_root, \"data\", \"stimuli\") \n",
    "videos_dir = op.join(stimuli, \"videos\")\n",
    "frames_dir = op.join(stimuli, \"frames\")\n",
    "annotation_file = op.join(frames_dir, 'annotations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae14828-28a1-44b5-9dee-022726693a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "from Buzznauts.data.utils import plot_video_frames\n",
    "from Buzznauts.data.videodataframe import VideoFrameDataset, ImglistToTensor, FrameDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94706b-5769-4c56-9115-d0865e8a6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Buzznauts.utils import seed_worker, set_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519f2ef-4401-49ca-b461-30a3f92b0105",
   "metadata": {},
   "source": [
    "### Demo 1 - Sampled Frames, without Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b19d6-06b1-40d4-a290-e5bf8944be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFrameDataset(\n",
    "    root_path=frames_dir,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=3,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=None,\n",
    "    random_shift=True,\n",
    "    test_mode=False)\n",
    "\n",
    "sample = dataset[0]\n",
    "frames = sample[0]  # list of PIL images\n",
    "label = sample[1]   # integer label\n",
    "\n",
    "plot_video_frames(rows=1, cols=3, frame_list=frames, plot_width=15., plot_height=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d1a9f-b006-4f94-8d06-9451d6f5c5bf",
   "metadata": {},
   "source": [
    "### Demo 2 - Single Continuous Frame Clip instead of Sampled Frames, without Image Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbb416-25a2-450e-9a16-121780bdd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFrameDataset(\n",
    "        root_path=frames_dir,\n",
    "        annotationfile_path=annotation_file,\n",
    "        num_segments=1,\n",
    "        frames_per_segment=9,\n",
    "        imagefile_template='img_{:05d}.jpg',\n",
    "        transform=None,\n",
    "        random_shift=True,\n",
    "        test_mode=False)\n",
    "\n",
    "sample = dataset[5]\n",
    "frames = sample[0]  # list of PIL images\n",
    "label = sample[1]  # integer label\n",
    "\n",
    "plot_video_frames(rows=3, cols=3, frame_list=frames, plot_width=10., plot_height=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692a013-ba1f-4d83-948f-6efe0087acc9",
   "metadata": {},
   "source": [
    "### Demo 3 - Sampled Frames, with Image Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9305e-caef-4016-8bd7-79a7e4f231aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(video_tensor):\n",
    "    \"\"\"Undoes mean/standard deviation normalization, zero to one scaling, and channel rearrangement for a batch of images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    video_tensor : tensor.FloatTensor \n",
    "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    video_array : numpy.ndarray[float]\n",
    "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) numpy array of floats\n",
    "    \"\"\"\n",
    "    inverse_normalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225])\n",
    "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241be228-1e81-4a48-b8ed-dcb311b93c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 5\n",
    "frames_per_segment = 1\n",
    "total_frames = num_segments * frames_per_segment\n",
    "\n",
    "# As of torchvision 0.8.0, torchvision transforms support batches of images\n",
    "# of size (BATCH x CHANNELS x HEIGHT x WIDTH) and apply deterministic or random\n",
    "# transformations on the batch identically on all images of the batch. Any torchvision\n",
    "# transform for image augmentation can thus also be used  for video augmentation.\n",
    "normalize = transforms.Compose([\n",
    "    ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    transforms.Resize(128), # image batch, resize smaller edge to 128\n",
    "    transforms.CenterCrop((100, 128)), # image batch, center crop to square 128x128\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.78125, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "dataset_without_preprocessing = VideoFrameDataset(\n",
    "    root_path=frames_dir,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=num_segments,\n",
    "    frames_per_segment=frames_per_segment,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=normalize,\n",
    "    random_shift=False,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "dataset_with_preprocessing = FrameDataset(\n",
    "    videoframedataset=dataset_without_preprocessing,\n",
    "    transform=preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d6989-7577-4329-95e5-eee1e87d5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_with_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1a84c-f0a7-4636-b396-182a6bf17a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample without Preprocessing')\n",
    "print('----------------------------')\n",
    "sample_without_preprocessing = dataset_without_preprocessing[2]\n",
    "frame_tensor = sample_without_preprocessing[0]  # tensor of shape (NUM_SEGMENTS*FRAMES_PER_SEGMENT) x CHANNELS x HEIGHT x WIDTH\n",
    "frame_array = denormalize(frame_tensor)\n",
    "plot_video_frames(rows=1, cols=5, frame_list=frame_array, plot_width=15., plot_height=3.)\n",
    "\n",
    "print('Sample with Preprocessing')\n",
    "print('-------------------------')\n",
    "frame_tensor = torch.stack([dataset_with_preprocessing[i][0] for i in range(2*total_frames, 3*total_frames)], dim=0)\n",
    "frame_array = denormalize(frame_tensor)\n",
    "plot_video_frames(rows=1, cols=5, frame_list=frame_array, plot_width=15., plot_height=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1372bb-6453-4a35-a928-ae12eba6d63e",
   "metadata": {},
   "source": [
    "### Demo 4 - Sampled Frames Dataloader, with Image Transforms and Dalaloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d7566-6595-4596-a467-e762ede9b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=set_generator())\n",
    "\n",
    "for epoch in range(10):\n",
    "    for video_batch, labels in dataloader:\n",
    "        \"\"\"\n",
    "        Insert Training Code Here\n",
    "        \"\"\"\n",
    "        print(labels)\n",
    "        print(\"\\nVideo Frames Batch Tensor Size:\", video_batch.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ae3fb-7ed6-4621-8b9e-0d6e82465c88",
   "metadata": {},
   "source": [
    "### Demo 5 - K-fold Cross-Validation with Sampled Frames Dataloader and Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2abff2-0a10-4de6-b551-b5e89f8945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232a5d3-d995-457a-99bc-cef130b43050",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 5\n",
    "frames_per_segment = 6\n",
    "total_frames = num_segments * frames_per_segment\n",
    "\n",
    "# As of torchvision 0.8.0, torchvision transforms support batches of images\n",
    "# of size (BATCH x CHANNELS x HEIGHT x WIDTH) and apply deterministic or random\n",
    "# transformations on the batch identically on all images of the batch. Any torchvision\n",
    "# transform for image augmentation can thus also be used  for video augmentation.\n",
    "normalize = transforms.Compose([\n",
    "    ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    transforms.Resize(128), # image batch, resize smaller edge to 128\n",
    "    transforms.CenterCrop((100, 128)), # image batch, center crop to square 128x128\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.78125, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "dataset_without_preprocessing = VideoFrameDataset(\n",
    "    root_path=frames_dir,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=num_segments,\n",
    "    frames_per_segment=frames_per_segment,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=normalize,\n",
    "    random_shift=False,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "dataset = FrameDataset(\n",
    "    videoframedataset=dataset_without_preprocessing,\n",
    "    transform=preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8475b-f614-43ee-8b77-163b1e6713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = '/content/drive/MyDrive/Buzznauts/data/pretrained/vaegan_enc_weights.pickle'\n",
    "\n",
    "def reset_weights(model, pretrained_path):\n",
    "    \"\"\"Try resetting model weights to avoid weight leakage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.Module\n",
    "    \"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "            \n",
    "    pretrained = load_vaegan_weights(model, pretrained_path)\n",
    "    model.load_my_state_dict(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96768bb3-0b29-42bb-ba9d-f2deb63a3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 5\n",
    "num_epochs = 1\n",
    "K_VAE = 1024 # size of the latent space vector\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of idx, no replacement\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=64,\n",
    "        sampler=train_subsampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=set_generator())\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=64,\n",
    "        sampler=val_subsampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=set_generator())\n",
    "    \n",
    "    # Init the neural network\n",
    "    \n",
    "    network = ConvVarAutoEncoder(K=K_VAE)\n",
    "    network.apply(reset_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Buzznauts)",
   "language": "python",
   "name": "buzznauts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
