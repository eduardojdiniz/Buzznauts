{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "import os\n",
    "import os.path as op\n",
    "from pathlib import Path\n",
    "import Buzznauts as buzz\n",
    "buzz_root = Path(buzz.__path__[0]).parent.absolute()\n",
    "\n",
    "# Data paths\n",
    "fmri_dir = op.join(buzz_root, \"data\", \"fmri\")\n",
    "stimuli = op.join(buzz_root, \"data\", \"stimuli\") \n",
    "videos_dir = op.join(stimuli, \"videos\")\n",
    "frames_dir = op.join(stimuli, \"frames\")\n",
    "annotation_file = op.join(frames_dir, 'annotations.txt')\n",
    "pretrained_dir = op.join(buzz_root, \"data\", \"pretrained\")\n",
    "pretrained_vaegan = op.join(pretrained_dir, \"vaegan_enc_weights.pickle\")\n",
    "                            \n",
    "# Visualizations path\n",
    "viz_dir = op.join(buzz_root, \"visualizations\")\n",
    "viz_vae_dir = op.join(viz_dir, \"vae\")\n",
    "\n",
    "# Model path\n",
    "models_dir = op.join(buzz_root, \"models\")\n",
    "model_vae_dir = op.join(models_dir, \"vae\")\n",
    "\n",
    "# Results paths\n",
    "results_dir = op.join(buzz_root, \"results\", \"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import interactive tools\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Buzznauts.utils import set_seed, set_device, seed_worker, set_generator\n",
    "from Buzznauts.data.utils import plot_video_frames\n",
    "from Buzznauts.data.videodataframe import VideoFrameDataset, ImglistToTensor, FrameDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVarAutoEncoder(nn.Module):\n",
    "    def __init__(self, K, data_shape=(3, 128, 128), num_filters=[192, 256, 384, 512, 768], filter_size=3):\n",
    "        super(ConvVarAutoEncoder, self).__init__()\n",
    "        ## 5 Conv Layers\n",
    "        filter_reduction = 5 * (filter_size // 2)\n",
    "\n",
    "        self.shape_after_conv = calc_output_size(data_shape, filter_size, num_filters)\n",
    "\n",
    "        self.flat_shape = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
    "\n",
    "        # Double for each additional layer of Conv\n",
    "        flat_size_after_conv = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
    "\n",
    "        # ENCODER\n",
    "        self.q_bias = BiasLayer(data_shape)\n",
    "        self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters[0], filter_size)\n",
    "        self.q_conv_2 = nn.Conv2d(num_filters[0], num_filters[1], filter_size)\n",
    "        self.q_conv_3 = nn.Conv2d(num_filters[1], num_filters[2], filter_size)\n",
    "        self.q_conv_4 = nn.Conv2d(num_filters[2], num_filters[3], filter_size)\n",
    "        self.q_conv_5 = nn.Conv2d(num_filters[3], num_filters[4], filter_size)\n",
    "        self.q_flatten = nn.Flatten()\n",
    "        self.q_fc_phi = nn.Linear(self.flat_shape, K+1)\n",
    "\n",
    "        # DECODER\n",
    "        self.p_fc_upsample = nn.Linear(K, self.flat_shape)\n",
    "        self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
    "        self.p_deconv_1 = nn.ConvTranspose2d(num_filters[4], num_filters[3], filter_size)\n",
    "        self.p_deconv_2 = nn.ConvTranspose2d(num_filters[3], num_filters[2], filter_size)\n",
    "        self.p_deconv_3 = nn.ConvTranspose2d(num_filters[2], num_filters[1], filter_size)\n",
    "        self.p_deconv_4 = nn.ConvTranspose2d(num_filters[1], num_filters[0], filter_size)\n",
    "        self.p_deconv_5 = nn.ConvTranspose2d(num_filters[0], data_shape[0], filter_size)\n",
    "\n",
    "        self.p_bias = BiasLayer(data_shape)\n",
    "\n",
    "        # Define a special extra parameter to learn scalar sig_x for all pixels\n",
    "        self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
    "\n",
    "\n",
    "    def infer(self, x):\n",
    "        \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
    "        rsample to get z\n",
    "        \"\"\"\n",
    "        s = self.q_bias(x)\n",
    "        s = F.elu(self.q_conv_1(s))\n",
    "        s = F.elu(self.q_conv_2(s))\n",
    "        s = F.elu(self.q_conv_3(s))\n",
    "        s = F.elu(self.q_conv_4(s))\n",
    "        s = F.elu(self.q_conv_5(s))\n",
    "        flat_s = s.view(s.size()[0], -1)\n",
    "        phi = self.q_fc_phi(flat_s)\n",
    "        return phi\n",
    "\n",
    "\n",
    "    def generate(self, zs):\n",
    "        \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
    "        \"\"\"\n",
    "        # Note that for the purposes of passing through the generator, we need\n",
    "        # to reshape zs to be size [b*n,k]\n",
    "        b, n, k = zs.size()\n",
    "        s = zs.view(b*n, -1)\n",
    "        s = F.elu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
    "        s = F.elu(self.p_deconv_1(s))\n",
    "        s = F.elu(self.p_deconv_2(s))\n",
    "        s = F.elu(self.p_deconv_3(s))\n",
    "        s = F.elu(self.p_deconv_4(s))\n",
    "        s = self.p_deconv_5(s)\n",
    "        s = self.p_bias(s)\n",
    "        mu_xs = s.view(b, n, -1)\n",
    "        return mu_xs\n",
    "\n",
    "    \n",
    "    def decode(self, zs):\n",
    "        # Included for compatability with conv-AE code\n",
    "        return self.generate(zs.unsqueeze(0))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # VAE.forward() is not used for training, but we'll treat it like a\n",
    "        # classic autoencoder by taking a single sample of z ~ q\n",
    "        phi = self.infer(x)\n",
    "        zs = rsample(phi, 1)\n",
    "        return self.generate(zs).view(x.size())\n",
    "    \n",
    "\n",
    "    def elbo(self, x, n=1):\n",
    "        \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
    "        samples of z\n",
    "        \"\"\"\n",
    "        phi = self.infer(x)\n",
    "        zs = rsample(phi, n)\n",
    "        mu_xs = self.generate(zs)\n",
    "        return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
    "\n",
    "    \n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        curr_state=self.state_dict()\n",
    "\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in curr_state:\n",
    "                continue\n",
    "            if isinstance(param, torch.Tensor):\n",
    "                param = param.data\n",
    "            curr_state[name].copy_(param)\n",
    "    \n",
    "    \n",
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(BiasLayer, self).__init__()\n",
    "        init_bias = torch.zeros(shape)\n",
    "        self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "    \n",
    "def calc_output_size(input_size, kernel_size, kchannels, padding=0, stride=1):\n",
    "    output_size = input_size\n",
    "    for kc in kchannels:\n",
    "        output_height = (output_size[1] + padding + padding - kernel_size) / (stride) + 1\n",
    "        output_width = (output_size[2] + padding + padding - kernel_size) / (stride) + 1\n",
    "\n",
    "        output_size = [kc, int(output_height), int(output_width)]\n",
    "\n",
    "    return tuple(output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO loss helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_q_p(zs, phi):\n",
    "    \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
    "    phi must be size [b,k+1]\n",
    "\n",
    "    This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
    "    just -1/2*(zs**2)\n",
    "    \"\"\"\n",
    "    b, n, k = zs.size()\n",
    "    mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
    "    log_p = -0.5*(zs**2)\n",
    "    log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
    "    # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
    "    return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
    "\n",
    "\n",
    "def log_p_x(x, mu_xs, sig_x):\n",
    "    \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
    "    pixel-wise log Gaussian probability\n",
    "\n",
    "    Sum over pixel dimensions, but mean over batch and samples.\n",
    "    \"\"\"\n",
    "    b, n = mu_xs.size()[:2]\n",
    "    # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
    "    # implicitly expanded when combined with mu_xs\n",
    "    x = x.reshape(b, 1, -1)\n",
    "    _, _, p = x.size()\n",
    "    squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
    "\n",
    "    # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
    "    # Expected value requires mean over [n]. Handling different size batches\n",
    "    # requires mean over [b].\n",
    "    return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
    "\n",
    "\n",
    "def rsample(phi, n_samples):\n",
    "    \"\"\"Sample z ~ q(z;phi)\n",
    "    Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
    "    entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "    standard deviation\n",
    "    \"\"\"\n",
    "    b, kplus1 = phi.size()\n",
    "    k = kplus1-1\n",
    "    mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
    "    eps = torch.randn(b, n_samples, k, device=phi.device)\n",
    "    return eps*sig.view(b,1,1) + mu.view(b,1,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vaegan_weights(model, pretrained_path):\n",
    "    # load pretrained weights\n",
    "    pretrained_fn = open(pretrained_path,'rb')\n",
    "    pretrained = pickle.load(pretrained_fn)\n",
    "\n",
    "    # have a look what's in the pretrained file\n",
    "    old_keynames=[]\n",
    "    for key, value in pretrained.items():\n",
    "        old_keynames.append(key) \n",
    "\n",
    "    # get the keynames of our model\n",
    "    curr_state=model.state_dict()\n",
    "    new_keynames=[]\n",
    "    for key, value in curr_state.items():\n",
    "        if key.startswith('q_conv'):\n",
    "            new_keynames.append(key)\n",
    "\n",
    "    # change the names of the pretrained model to match our model\n",
    "    for i in range(len(old_keynames)):\n",
    "        pretrained[new_keynames[i]] = pretrained[old_keynames[i]]\n",
    "        del pretrained[old_keynames[i]]\n",
    "\n",
    "    # change size & make the weights a torch\n",
    "    # In TF, Conv2d filter shape is [filter_height, filter_width, in_channels, out_channels],\n",
    "    # while in Pytorch is (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "    # So we need to permute [3,2,0,1]\n",
    "    for key, value in pretrained.items():\n",
    "        if len(value.shape)==4:\n",
    "            new_val=torch.tensor(value)\n",
    "            new_val=new_val.permute(3,2,0,1)\n",
    "        else: \n",
    "            new_val=torch.tensor(value)\n",
    "    \n",
    "        pretrained[key] = new_val\n",
    "    \n",
    "    return pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    \"\"\"Try resetting model weights to avoid weight leakage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.Module\n",
    "    \"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# Set seed to the random generators to ensure reproducibility\n",
    "seed = set_seed()\n",
    "\n",
    "# Set computational device (cuda if GPU is available, else cpu)\n",
    "device = set_device()\n",
    "      \n",
    "# Number of folds for cross-validation \n",
    "k_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10 \n",
    "\n",
    "# Batch size\n",
    "batch_size = 32 \n",
    "\n",
    "# Size of the VAE's latent space  \n",
    "K_VAE = 128 \n",
    "\n",
    "#---------------\n",
    "# Create Dataset\n",
    "#---------------\n",
    "\n",
    "# Number of splits in each video\n",
    "num_segments = 5\n",
    "\n",
    "# Number of frames per split\n",
    "frames_per_segment = 6\n",
    "\n",
    "# Total number of training frames\n",
    "total_frames = num_segments * frames_per_segment\n",
    "\n",
    "# Frame size\n",
    "frame_size = 32 \n",
    "width = frame_size\n",
    "height = frame_size\n",
    "\n",
    "# Num of channels\n",
    "num_channels = 3\n",
    "\n",
    "# Data shape\n",
    "data_shape = (num_channels, frame_size, frame_size)\n",
    "\n",
    "# Tensorize convert PIL images to tensors and resize each frame to frame_size\n",
    "tensorize = transforms.Compose([\n",
    "    ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    transforms.Resize(frame_size), # image batch, resize smaller edge to 128\n",
    "])\n",
    "\n",
    "# Preprocess center crop to 100x128, normalize and apply random affine\n",
    "# and horizontal flips to each frame\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop((frame_size, frame_size)), # image batch, center crop to square 100x128\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.78125, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "# Videoframe dataset: each sample is of size (FRAMES X CHANNELS X HEIGHT X WIDTH)\n",
    "videoframe_dataset = VideoFrameDataset(\n",
    "    root_path=frames_dir,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=num_segments,\n",
    "    frames_per_segment=frames_per_segment,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=tensorize,\n",
    "    random_shift=False,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "# Frame dataset: each sample is of size (CHANNELS X HEIGHT X WIDTH)\n",
    "frame_dataset = FrameDataset(\n",
    "    videoframedataset=videoframe_dataset,\n",
    "    transform=preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loss values during training for each fold\n",
    "loss_train = {f'Fold_{i}': [] for i in range(1, k_folds+1)}\n",
    "# Save loss during validation for each fold\n",
    "loss_val = {f'Fold_{i}': [] for i in range(1, k_folds+1)}\n",
    "# Save overall loss during validation for each fold\n",
    "loss_val_overall = {f'Fold_{i}': None for i in range(1, k_folds+1)}\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(frame_dataset)):\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('-------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of idx, no replacement\n",
    "    train_subsampler = SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=frame_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_subsampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=set_generator())\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset=frame_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_subsampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=set_generator())\n",
    "    \n",
    "    # Instantiate network\n",
    "    convVAE = ConvVarAutoEncoder(data_shape=data_shape, K=K_VAE)\n",
    "    convVAE.apply(reset_weights)\n",
    "    pretrained = load_vaegan_weights(convVAE, pretrained_vaegan)\n",
    "    convVAE.load_my_state_dict(pretrained)\n",
    "    \n",
    "    # Freezing layers\n",
    "    freeze_idx = [2, 3, 4, 5]\n",
    "    for idx, param in enumerate(convVAE.parameters()): \n",
    "        if idx in freeze_idx: param.requires_grad = False\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, convVAE.parameters()),\n",
    "                                 lr=3e-4, weight_decay=0)\n",
    "    \n",
    "    convVAE.to(device)\n",
    "    convVAE.train()\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in trange(num_epochs, desc='Epochs'):\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, (frame, label) in enumerate(tqdm(train_loader, \n",
    "                                                total=len(train_loader) // batch_size,\n",
    "                                                desc='Batches', leave=False)):\n",
    "            frame = frame.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = -convVAE.elbo(frame)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Saving loss\n",
    "            loss_train[f'Fold_{fold+1}'].append(-loss.item())\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 100))\n",
    "                current_loss = 0.0\n",
    "        \n",
    "    # Evaluation for this fold\n",
    "    convVAE.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the DataLoader for validation data\n",
    "        for i, (frame, label) in enumerate(tqdm(val_loader, \n",
    "                                                total=len(val_loader) // batch_size,\n",
    "                                                desc='Batches', leave=False)):\n",
    "            # Compute loss\n",
    "            loss = -convVAE.elbo(frame)\n",
    "\n",
    "            # Saving loss\n",
    "            loss_val[f'Fold_{fold+1}'].append(-loss.item())\n",
    "\n",
    "        # Print overall fold loss \n",
    "        loss_val_overall[f'Fold_{fold+1}'] = sum(loss_val[f'Fold_{fold+1}'])\n",
    "        print('Total loss for fold %d: %d %%' % (fold, results[f'Fold_{fold+1}']))\n",
    "        print('--------------------------------')\n",
    "    \n",
    "# Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('----------------------------------------------------')\n",
    "overall_sum = 0.0\n",
    "for key, value in loss_val_overall.items():\n",
    "    print(f'Fold {key+1}: {value} %')\n",
    "    overall_sum += value\n",
    "print(f'Average: {overall_sum/len(loss_val_overall.items())} %')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch (Buzznauts)",
   "language": "python",
   "name": "buzznauts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05251c6ee2824476825857669474bbd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b12c4866024855b96441d9af95158b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23681221c03c41eeb621a56f7b7ef037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d2f4ec50ef4091a5ca222a61c1ce48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36e95b8e5c0f4fc49eebe988cdc69657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45110124517b4a849eecb54ba214483b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b4ff4a735d74741a16c28631eeb0e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b7665c14274ce5820be74acc04b48c",
      "placeholder": "​",
      "style": "IPY_MODEL_a789ff430c704c88b424e3c703837826",
      "value": " 0/1 [00:00&lt;?, ?it/s]"
     }
    },
    "61f7542ccddb437786f81f8e2eaf64df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75beb26e5a90448884c99b52c13e403e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23681221c03c41eeb621a56f7b7ef037",
      "placeholder": "​",
      "style": "IPY_MODEL_36e95b8e5c0f4fc49eebe988cdc69657",
      "value": "Epochs:   0%"
     }
    },
    "859e4ea521ab43f6b114cc85c92ce364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93ec85a8904f4624a68cabc1f6f4c7ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44fa98329b9437f81289703de125680",
      "placeholder": "​",
      "style": "IPY_MODEL_859e4ea521ab43f6b114cc85c92ce364",
      "value": " 0/3125 [00:00&lt;?, ?it/s]"
     }
    },
    "9c8f9ce945f142a4b6c73bbf47331bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61f7542ccddb437786f81f8e2eaf64df",
      "placeholder": "​",
      "style": "IPY_MODEL_d03ad9486fae4c4b8710f8f72406fc6a",
      "value": "Batches:   0%"
     }
    },
    "a789ff430c704c88b424e3c703837826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0fc935f5caf486aafec9289828f0342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75beb26e5a90448884c99b52c13e403e",
       "IPY_MODEL_db2e31b42ee84fc68b70c48dd1804412",
       "IPY_MODEL_5b4ff4a735d74741a16c28631eeb0e89"
      ],
      "layout": "IPY_MODEL_24d2f4ec50ef4091a5ca222a61c1ce48"
     }
    },
    "b60b63f8845d40b983df3bbecda64574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce6482f2519f41a5897da75db74951cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05251c6ee2824476825857669474bbd9",
      "max": 3125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b60b63f8845d40b983df3bbecda64574",
      "value": 0
     }
    },
    "d03ad9486fae4c4b8710f8f72406fc6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0b7665c14274ce5820be74acc04b48c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d44fa98329b9437f81289703de125680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db2e31b42ee84fc68b70c48dd1804412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5e38eac46734a63a4b8bbc1e232e2dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45110124517b4a849eecb54ba214483b",
      "value": 0
     }
    },
    "f3e5444497f04c3fb0448a2c9beca7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c8f9ce945f142a4b6c73bbf47331bcc",
       "IPY_MODEL_ce6482f2519f41a5897da75db74951cf",
       "IPY_MODEL_93ec85a8904f4624a68cabc1f6f4c7ad"
      ],
      "layout": "IPY_MODEL_20b12c4866024855b96441d9af95158b"
     }
    },
    "f5e38eac46734a63a4b8bbc1e232e2dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
