{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a   href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/demo_CrossValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "id": "21d8b141-b3fd-47bb-9b6d-d65067620d97"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Demo VideoDataFrame Class \n",
                "This demo uses the Algonauts dataset.\n",
                "    \n",
                "TABLE OF CODE CONTENTS:\n",
                "1. Minimal demo without image transforms\n",
                "2. Minimal demo without sparse temporal sampling for single continuous frame clips, without image transforms\n",
                "3. Demo with image transforms\n",
                "4. Demo with image transforms and dataloader\n",
                "\n",
                "For more details about the VideoDataFrame Class, see the [VideoDataset Repo](https://video-dataset-loading-pytorch.readthedocs.io/en/latest/VideoDataset.html)"
            ],
            "id": "297cba83-73af-4139-a22e-accb576fd93d"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup "
            ],
            "id": "dd1fc245-da1a-45da-ae03-4e422e0e199d"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Download Buzznauts\n",
                "!pip install duecredit --quiet\n",
                "!pip install git+https://github.com/eduardojdiniz/Buzznauts --quiet"
            ],
            "id": "98a50609-1ebf-4460-8833-67b5947a9a07",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount(\"/content/drive\")"
            ],
            "id": "e3f1f26e-b3d6-44b6-9d54-baac0a07ba41",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Load Buzznauts data functions\n",
                "!pip install decord --quiet\n",
                "!pip install nilearn\n",
                "from Buzznauts.data.utils import plot_video_frames\n",
                "from Buzznauts.data.videodataframe import VideoFrameDataset, ImglistToTensor"
            ],
            "id": "dae14828-28a1-44b5-9dee-022726693a53",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Import pytorch\n",
                "from torchvision import transforms\n",
                "import torch"
            ],
            "id": "7acaa35b-9ab9-4f22-9a13-8457a0f6db6a",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Set videos and annotation file path\n",
                "import os.path as op\n",
                "stimuli = \"/content/drive/MyDrive/Buzznauts/data/stimuli\"\n",
                "videos_root = op.join(stimuli, \"frames\")\n",
                "annotation_file = op.join(videos_root, \"annotations.txt\")"
            ],
            "id": "289baca9-2d32-44cd-a5ca-d43e0cd2d576",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def denormalize(video_tensor):\n",
                "    \"\"\"Undoes mean/standard deviation normalization, zero to one scaling, and channel rearrangement for a batch of images.\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    video_tensor : tensor.FloatTensor \n",
                "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
                "        \n",
                "    Returns\n",
                "    ----------\n",
                "    video_array : numpy.ndarray[float]\n",
                "        A (FRAMES x CHANNELS x HEIGHT x WIDTH) numpy array of floats\n",
                "    \"\"\"\n",
                "    inverse_normalize = transforms.Normalize(\n",
                "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
                "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225])\n",
                "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()"
            ],
            "id": "694b19d6-06b1-40d4-a290-e5bf8944be13",
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cross Validation Example"
            ],
            "id": "9b1372bb-6453-4a35-a928-ae12eba6d63e"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "preprocess = transforms.Compose([\n",
                "    ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
                "    transforms.Resize(299),  # image batch, resize smaller edge to 299\n",
                "    transforms.CenterCrop(299),  # image batch, center crop to square 299x299\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "])\n",
                "\n",
                "# Get the 1000 training examples\n",
                "annotation_file_train = op.join(videos_root, \"annotations_train.txt\") \n",
                "\n",
                "dataset = VideoFrameDataset(\n",
                "    root_path=videos_root,\n",
                "    annotationfile_path=annotation_file_train,\n",
                "    num_segments=5,\n",
                "    frames_per_segment=1,\n",
                "    imagefile_template='img_{:05d}.jpg',\n",
                "    transform=preprocess,\n",
                "    random_shift=True,\n",
                "    test_mode=False\n",
                ")\n",
                "\n",
                "from torch.utils.data import random_split\n",
                "CV = 5\n",
                "epochs = 1 \n",
                "n_samples = 10\n",
                "train_len = 100\n",
                "val_len = len(dataset) - train_len\n",
                "\n",
                "for cv in range(CV):\n",
                "\n",
                "  train_dataset , val_dataset = random_split(dataset, [train_len, val_len])\n",
                "  val_dataset.dataset.test_mode = True\n",
                "\n",
                "  for epoch in range(10):\n",
                "      for video_batch, labels in train_dataset:\n",
                "          \"\"\"\n",
                "          Insert Training Code Here\n",
                "          \"\"\"\n",
                "          print(labels)\n",
                "          frames = denormalize(video_batch)\n",
                "          plot_video_frames(rows=1, cols=5, frame_list=frames, plot_width=15., plot_height=3.)\n",
                "\n",
                "          break\n",
                "      break"
            ],
            "id": "tn4DVr5AxxMZ",
            "execution_count": null,
            "outputs": []
        }
    ]
}
