{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdB66FX2dUq2"
      },
      "source": [
        "<a   href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imi5aU8c7a6x",
        "outputId": "4869e983-4f18-4207-b3e2-ca1b4dd4cbeb"
      },
      "source": [
        "!pip install duecredit --quiet\n",
        "!git clone https://github.com/eduardojdiniz/Buzznauts --quiet\n",
        "!pip install torchinfo --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 30 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 51 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 61 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 86 kB 2.6 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 122 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 133 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 143 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 153 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 163 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 179 kB 30.3 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeP5YOo3dTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190f3a7f-4a4b-4139-8e3c-f7c4322d1ed2"
      },
      "source": [
        "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
        "# Imports\n",
        "import torch\n",
        "import random\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os.path as op"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL557EF0scbI",
        "outputId": "ebd6c630-b202-479a-eca5-12ed07c8b863"
      },
      "source": [
        "# @title Download MNIST and CIFAR10 datasets\n",
        "import tarfile, requests, os\n",
        "\n",
        "fname = 'MNIST.tar.gz'\n",
        "name = 'mnist'\n",
        "url = 'https://osf.io/y2fj6/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading MNIST dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading MNIST completed..\\n')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall(name)\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('MNIST dataset has been dowloaded.\\n')\n",
        "\n",
        "\n",
        "fname = 'cifar-10-python.tar.gz'\n",
        "name = 'cifar10'\n",
        "url = 'https://osf.io/jbpme/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading CIFAR10 dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading CIFAR10 completed.')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall(name)\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('CIFAR10 dataset has been dowloaded.')\n",
        "  \n",
        "\n",
        "# @markdown Load MNIST and CIFAR10 image datasets\n",
        "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
        "\n",
        "# MNIST\n",
        "mnist = datasets.MNIST('./mnist/',\n",
        "                       train=True,\n",
        "                       transform=transforms.ToTensor(),\n",
        "                       download=False)\n",
        "mnist_val = datasets.MNIST('./mnist/',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=False)\n",
        "\n",
        "# CIFAR 10\n",
        "cifar10 = datasets.CIFAR10('./cifar10/',\n",
        "                           train=True,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=False)\n",
        "cifar10_val = datasets.CIFAR10('./cifar10/',\n",
        "                               train=False,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=False)\n",
        "\n",
        "def get_data(name='mnist'):\n",
        "  if name == 'mnist':\n",
        "    my_dataset_name = \"MNIST\"\n",
        "    my_dataset = mnist\n",
        "    my_valset = mnist_val\n",
        "    my_dataset_shape = (1, 28, 28)\n",
        "    my_dataset_size = 28 * 28\n",
        "  elif name == 'cifar10':\n",
        "    my_dataset_name = \"CIFAR10\"\n",
        "    my_dataset = cifar10\n",
        "    my_valset = cifar10_val\n",
        "    my_dataset_shape = (3, 32, 32)\n",
        "    my_dataset_size = 3 * 32 * 32\n",
        "\n",
        "  return my_dataset, my_dataset_name, my_dataset_shape, my_dataset_size, my_valset\n",
        "\n",
        "\n",
        "train_set, dataset_name, data_shape, data_size, valid_set = get_data(name='mnist')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading MNIST dataset...\n",
            "\n",
            "Downloading MNIST completed..\n",
            "\n",
            "\n",
            "Downloading CIFAR10 dataset...\n",
            "\n",
            "Downloading CIFAR10 completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqJGvpAf3zhJ"
      },
      "source": [
        "class BiasLayer(nn.Module):\n",
        "  def __init__(self, shape):\n",
        "    super(BiasLayer, self).__init__()\n",
        "    init_bias = torch.zeros(shape)\n",
        "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.bias\n",
        "\n",
        "\n",
        "def cout(x, layer):\n",
        "  \"\"\"Unnecessarily complicated but complete way to\n",
        "  calculate the output depth, height and width size for a Conv2D layer\n",
        "\n",
        "  Args:\n",
        "    x (tuple): input size (depth, height, width)\n",
        "    layer (nn.Conv2d): the Conv2D layer\n",
        "\n",
        "  returns:\n",
        "    (int): output shape as given in [Ref]\n",
        "\n",
        "  Ref:\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "  \"\"\"\n",
        "  assert isinstance(layer, nn.Conv2d)\n",
        "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
        "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
        "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
        "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
        "  in_depth, in_height, in_width = x\n",
        "  out_depth = layer.out_channels\n",
        "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
        "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
        "  return (out_depth, out_height, out_width)\n",
        "\n",
        "\n",
        "# @title Helper functions\n",
        "\n",
        "#@title Helper functions\n",
        "\n",
        "def image_moments(image_batches, n_batches=None):\n",
        "  \"\"\"\n",
        "  Compute mean an covariance of all pixels from batches of images\n",
        "  \"\"\"\n",
        "  m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n",
        "  n = 0\n",
        "  for im in tqdm(image_batches, total=n_batches, leave=False,\n",
        "                 desc='Computing pixel mean and covariance...'):\n",
        "    im = im.to(DEVICE)\n",
        "    b = im.size()[0]\n",
        "    im = im.view(b, -1)\n",
        "    m1 = m1 + im.sum(dim=0)\n",
        "    m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n",
        "    n += b\n",
        "  m1, m2 = m1/n, m2/n\n",
        "  cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n",
        "  return m1.cpu(), cov.cpu()\n",
        "\n",
        "\n",
        "def interpolate(A, B, num_interps):\n",
        "  if A.shape != B.shape:\n",
        "    raise ValueError('A and B must have the same shape to interpolate.')\n",
        "  alphas = np.linspace(0, 1, num_interps)\n",
        "  return np.array([(1-a)*A + a*B for a in alphas])\n",
        "\n",
        "\n",
        "def kl_q_p(zs, phi):\n",
        "  \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
        "  phi must be size [b,k+1]\n",
        "\n",
        "  This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
        "  just -1/2*(zs**2)\n",
        "  \"\"\"\n",
        "  b, n, k = zs.size()\n",
        "  mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
        "  log_p = -0.5*(zs**2)\n",
        "  log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
        "  # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
        "  return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def log_p_x(x, mu_xs, sig_x):\n",
        "  \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
        "  pixel-wise log Gaussian probability\n",
        "\n",
        "  Sum over pixel dimensions, but mean over batch and samples.\n",
        "  \"\"\"\n",
        "  b, n = mu_xs.size()[:2]\n",
        "  # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
        "  # implicitly expanded when combined with mu_xs\n",
        "  x = x.reshape(b, 1, -1)\n",
        "  _, _, p = x.size()\n",
        "  squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
        "\n",
        "  # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
        "  # Expected value requires mean over [n]. Handling different size batches\n",
        "  # requires mean over [b].\n",
        "  return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def pca_encoder_decoder(mu, cov, k):\n",
        "  \"\"\"\n",
        "  Compute encoder and decoder matrices for PCA dimensionality reduction\n",
        "  \"\"\"\n",
        "  mu = mu.view(1,-1)\n",
        "  u, s, v = torch.svd_lowrank(cov, q=k)\n",
        "  W_encode = v / torch.sqrt(s)\n",
        "  W_decode = u * torch.sqrt(s)\n",
        "\n",
        "  def pca_encode(x):\n",
        "    # Encoder: subtract mean image and project onto top K eigenvectors of\n",
        "    # the data covariance\n",
        "    return (x.view(-1,mu.numel()) - mu) @ W_encode\n",
        "\n",
        "  def pca_decode(h):\n",
        "    # Decoder: un-project then add back in the mean\n",
        "    return (h @ W_decode.T) + mu\n",
        "\n",
        "  return pca_encode, pca_decode\n",
        "\n",
        "\n",
        "def cout(x, layer):\n",
        "  \"\"\"Unnecessarily complicated but complete way to\n",
        "  calculate the output depth, height and width size for a Conv2D layer\n",
        "\n",
        "  Args:\n",
        "    x (tuple): input size (depth, height, width)\n",
        "    layer (nn.Conv2d): the Conv2D layer\n",
        "\n",
        "  returns:\n",
        "    (int): output shape as given in [Ref]\n",
        "\n",
        "  Ref:\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "  \"\"\"\n",
        "  assert isinstance(layer, nn.Conv2d)\n",
        "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
        "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
        "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
        "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
        "  in_depth, in_height, in_width = x\n",
        "  out_depth = layer.out_channels\n",
        "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
        "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
        "  return (out_depth, out_height, out_width)\n",
        "\n",
        "\n",
        "def load_vaegan_weights(model, pretrained_path):\n",
        "  # load pretrained weights\n",
        "  pretrained_fn = open(pretrained_path,'rb')\n",
        "  pretrained = pickle.load(pretrained_fn)\n",
        "\n",
        "  # have a look what's in the pretrained file\n",
        "  old_keynames=[]\n",
        "  for key, value in pretrained.items() :\n",
        "    print (key,value.shape) \n",
        "    old_keynames.append(key) \n",
        "\n",
        "  # get the keynames of our model\n",
        "  curr_state=model.state_dict()\n",
        "  new_keynames=[]\n",
        "  for key, value in curr_state.items() :\n",
        "      if key.startswith('q_conv'):\n",
        "        new_keynames.append(key)\n",
        "\n",
        "  # change the names of the pretrained model to match our model\n",
        "  for i in range(len(old_keynames)):\n",
        "    pretrained[new_keynames[i]] = pretrained[old_keynames[i]]\n",
        "    del pretrained[old_keynames[i]]\n",
        "\n",
        "  # change size & make the weights a torch\n",
        "  # In TF, Conv2d filter shape is [filter_height, filter_width, in_channels, out_channels],\n",
        "  # while in Pytorch is (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
        "  # SO we need to permute [3,2,0,1]\n",
        "  for key, value in pretrained.items() :\n",
        "    if len(value.shape)==4:\n",
        "      new_val=torch.tensor(value)\n",
        "      new_val=new_val.permute(3,2,0,1)\n",
        "    else: \n",
        "      new_val=torch.tensor(value)\n",
        "    \n",
        "    pretrained[key] = new_val\n",
        "\n",
        "  return pretrained"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWoN9EZf9j9Q"
      },
      "source": [
        "Convolutional Auto Encoder [FULL]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F929qabx7PXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71f790f-58b6-4fdb-dd5b-fe6dfe3efd07"
      },
      "source": [
        "K_VAE = 1024\n",
        "\n",
        "class ConvVarAutoEncoder(nn.Module):\n",
        "  def __init__(self, K, num_filters=[192, 256, 384, 512, 768], filter_size=3):\n",
        "    super(ConvVarAutoEncoder, self).__init__()\n",
        "    ## 5 Conv Layers\n",
        "    filter_reduction = 5 * (filter_size // 2)\n",
        "\n",
        "    self.shape_after_conv = calc_output_size(data_shape, filter_size, num_filters)\n",
        "    \n",
        "    self.flat_shape = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
        "    \n",
        "    # Double for each additional layer of Conv\n",
        "    flat_size_after_conv = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
        "\n",
        "    # ENCODER\n",
        "    self.q_bias = BiasLayer(data_shape)\n",
        "    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters[0], filter_size)\n",
        "    self.q_conv_2 = nn.Conv2d(num_filters[0], num_filters[1], filter_size)\n",
        "    self.q_conv_3 = nn.Conv2d(num_filters[1], num_filters[2], filter_size)\n",
        "    self.q_conv_4 = nn.Conv2d(num_filters[2], num_filters[3], filter_size)\n",
        "    self.q_conv_5 = nn.Conv2d(num_filters[3], num_filters[4], filter_size)\n",
        "    self.q_flatten = nn.Flatten()\n",
        "    self.q_fc_phi = nn.Linear(self.flat_shape, K+1)\n",
        "\n",
        "    # DECODER\n",
        "    self.p_fc_upsample = nn.Linear(K, self.flat_shape)\n",
        "    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
        "    self.p_deconv_1 = nn.ConvTranspose2d(num_filters[4], num_filters[3], filter_size)\n",
        "    self.p_deconv_2 = nn.ConvTranspose2d(num_filters[3], num_filters[2], filter_size)\n",
        "    self.p_deconv_3 = nn.ConvTranspose2d(num_filters[2], num_filters[1], filter_size)\n",
        "    self.p_deconv_4 = nn.ConvTranspose2d(num_filters[1], num_filters[0], filter_size)\n",
        "    self.p_deconv_5 = nn.ConvTranspose2d(num_filters[0], data_shape[0], filter_size)\n",
        "\n",
        "    self.p_bias = BiasLayer(data_shape)\n",
        "\n",
        "    # Define a special extra parameter to learn scalar sig_x for all pixels\n",
        "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "\n",
        "  def infer(self, x):\n",
        "    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
        "    rsample to get z\n",
        "    \"\"\"\n",
        "    s = self.q_bias(x)\n",
        "    s = F.elu(self.q_conv_1(s))\n",
        "    s = F.elu(self.q_conv_2(s))\n",
        "    s = F.elu(self.q_conv_3(s))\n",
        "    s = F.elu(self.q_conv_4(s))\n",
        "    s = F.elu(self.q_conv_5(s))\n",
        "    flat_s = s.view(s.size()[0], -1)\n",
        "    phi = self.q_fc_phi(flat_s)\n",
        "    return phi\n",
        "\n",
        "\n",
        "  def generate(self, zs):\n",
        "    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
        "    \"\"\"\n",
        "    # Note that for the purposes of passing through the generator, we need\n",
        "    # to reshape zs to be size [b*n,k]\n",
        "    b, n, k = zs.size()\n",
        "    s = zs.view(b*n, -1)\n",
        "    s = F.elu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
        "    s = F.elu(self.p_deconv_1(s))\n",
        "    s = F.elu(self.p_deconv_2(s))\n",
        "    s = F.elu(self.p_deconv_3(s))\n",
        "    s = F.elu(self.p_deconv_4(s))\n",
        "    s = self.p_deconv_5(s)\n",
        "    s = self.p_bias(s)\n",
        "    mu_xs = s.view(b, n, -1)\n",
        "    return mu_xs\n",
        "\n",
        "\n",
        "  def decode(self, zs):\n",
        "    # Included for compatability with conv-AE code\n",
        "    return self.generate(zs.unsqueeze(0))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # VAE.forward() is not used for training, but we'll treat it like a\n",
        "    # classic autoencoder by taking a single sample of z ~ q\n",
        "    phi = self.infer(x)\n",
        "    zs = rsample(phi, 1)\n",
        "    return self.generate(zs).view(x.size())\n",
        "\n",
        "\n",
        "  def elbo(self, x, n=1):\n",
        "    \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
        "    samples of z\n",
        "    \"\"\"\n",
        "    phi = self.infer(x)\n",
        "    zs = rsample(phi, n)\n",
        "    mu_xs = self.generate(zs)\n",
        "    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
        "\n",
        "  \n",
        "  def load_my_state_dict(self, state_dict):\n",
        "    curr_state=self.state_dict()\n",
        "    \n",
        "    for name, param in state_dict.items():\n",
        "      print(type(param))\n",
        "      if name not in curr_state:\n",
        "        print('name does not exist -- skipping')\n",
        "        continue\n",
        "      if isinstance(param, torch.Tensor):\n",
        "          param = param.data\n",
        "      curr_state[name].copy_(param)\n",
        "\n",
        "\n",
        "###============================FUNCTIONS=========================###\n",
        "\n",
        "\n",
        "def expected_z(phi):\n",
        "  return phi[:, :-1]\n",
        "\n",
        "\n",
        "def rsample(phi, n_samples):\n",
        "  \"\"\"Sample z ~ q(z;phi)\n",
        "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
        "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
        "  standard deviation\n",
        "  \"\"\"\n",
        "  b, kplus1 = phi.size()\n",
        "  k = kplus1-1\n",
        "  mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
        "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
        "  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n",
        "\n",
        "\n",
        "def train_vae(vae, dataset, epochs=10, n_samples=1000):\n",
        "  opt = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0)\n",
        "  elbo_vals = []\n",
        "  vae.to(DEVICE)\n",
        "  vae.train()\n",
        "  loader = DataLoader(dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
        "  for epoch in trange(epochs, desc='Epochs'):\n",
        "    for im, _ in tqdm(loader, total=len(dataset) // 64, desc='Batches', leave=False):\n",
        "      im = im.to(DEVICE)\n",
        "      opt.zero_grad()\n",
        "      loss = -vae.elbo(im)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      elbo_vals.append(-loss.item())\n",
        "  vae.to('cuda')\n",
        "  vae.eval()\n",
        "  return elbo_vals\n",
        "\n",
        "def calc_output_size(input_size, kernel_size, kchannels, padding=0, stride=1):\n",
        "  output_size = input_size\n",
        "  for kc in kchannels:\n",
        "    output_height = (output_size[1] + padding + padding - kernel_size) / (stride) + 1\n",
        "    output_width = (output_size[2] + padding + padding - kernel_size) / (stride) + 1\n",
        "\n",
        "    output_size = [kc, int(output_height), int(output_width)]\n",
        "    print(output_size)\n",
        "\n",
        "  return output_size\n",
        "\n",
        "                     \n",
        "\n",
        "\n",
        "\n",
        "convVAE = ConvVarAutoEncoder(K=K_VAE)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[192, 26, 26]\n",
            "[256, 24, 24]\n",
            "[384, 22, 22]\n",
            "[512, 20, 20]\n",
            "[768, 18, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfCmpcBv8Dlt",
        "outputId": "08e41298-60ff-4e6c-924c-7b6666b2d465"
      },
      "source": [
        "convVAE"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvVarAutoEncoder(\n",
              "  (q_bias): BiasLayer()\n",
              "  (q_conv_1): Conv2d(1, 192, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_2): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_4): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_5): Conv2d(512, 768, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (q_fc_phi): Linear(in_features=248832, out_features=1025, bias=True)\n",
              "  (p_fc_upsample): Linear(in_features=1024, out_features=248832, bias=True)\n",
              "  (p_unflatten): Unflatten(dim=-1, unflattened_size=[768, 18, 18])\n",
              "  (p_deconv_1): ConvTranspose2d(768, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_2): ConvTranspose2d(512, 384, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_3): ConvTranspose2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_4): ConvTranspose2d(256, 192, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_5): ConvTranspose2d(192, 1, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_bias): BiasLayer()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7xotQyjkA_6",
        "outputId": "1067c6ea-fec1-4da1-b69b-00c2bc58e63f"
      },
      "source": [
        "os = calc_output_size([3, 128, 128], 3, [192, 256, 384, 512, 768])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[192, 126, 126]\n",
            "[256, 124, 124]\n",
            "[384, 122, 122]\n",
            "[512, 120, 120]\n",
            "[768, 118, 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cBMaSMbphMI",
        "outputId": "fa753a3c-c978-4f92-a7fa-cb8bc7f33934"
      },
      "source": [
        "768 * 118 * 118"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10693632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac8t_FT48Und"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "# trained_CVAE = train_vae(convVAE, train_set, epochs = 1, n_samples = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "YB9zjh0UUXCK",
        "outputId": "88e85074-a8b6-4b00-a7c7-e24d67cc73c7"
      },
      "source": [
        "pretrained_path = '/content/drive/MyDrive/Buzznauts/data/pretrained/vaegan_enc_weights.pickle'\n",
        "pretrained = load_vaegan_weights(convVAE, pretrained_path)\n",
        "convVAE.load_my_state_dict(pretrained)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder/layer1/conv2d/W (3, 3, 3, 192)\n",
            "encoder/layer1/conv2d/b (192,)\n",
            "encoder/layer2/conv2d/W (3, 3, 192, 256)\n",
            "encoder/layer2/conv2d/b (256,)\n",
            "encoder/layer3/conv2d/W (3, 3, 256, 384)\n",
            "encoder/layer3/conv2d/b (384,)\n",
            "encoder/layer4/conv2d/W (3, 3, 384, 512)\n",
            "encoder/layer4/conv2d/b (512,)\n",
            "encoder/layer5/conv2d/W (3, 3, 512, 768)\n",
            "encoder/layer5/conv2d/b (768,)\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7a367809bb8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpretrained_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Buzznauts/data/pretrained/vaegan_enc_weights.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vaegan_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconvVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_my_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3448cc498e0f>\u001b[0m in \u001b[0;36mload_my_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [192, 1, 3, 3] doesn't match the broadcast shape [192, 3, 3, 3]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2qXRK9hfaB",
        "outputId": "b7d8c067-a270-446e-af48-40341683b21f"
      },
      "source": [
        "type(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.MNIST"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDmSS_f6hgot"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}