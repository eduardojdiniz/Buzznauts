{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdB66FX2dUq2"
      },
      "source": [
        "<a   href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imi5aU8c7a6x"
      },
      "source": [
        "!pip install duecredit --quiet\n",
        "!git clone https://github.com/eduardojdiniz/Buzznauts --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeP5YOo3dTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782755f3-6499-4e66-d2f1-2af933ef7027"
      },
      "source": [
        "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
        "# Imports\n",
        "import torch\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os.path as op"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL557EF0scbI",
        "outputId": "f7fd1805-8052-4eb0-b71c-941073c4596f"
      },
      "source": [
        "# @title Download MNIST and CIFAR10 datasets\n",
        "import tarfile, requests, os\n",
        "\n",
        "fname = 'MNIST.tar.gz'\n",
        "name = 'mnist'\n",
        "url = 'https://osf.io/y2fj6/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading MNIST dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading MNIST completed..\\n')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall(name)\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('MNIST dataset has been dowloaded.\\n')\n",
        "\n",
        "\n",
        "fname = 'cifar-10-python.tar.gz'\n",
        "name = 'cifar10'\n",
        "url = 'https://osf.io/jbpme/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading CIFAR10 dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading CIFAR10 completed.')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall(name)\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('CIFAR10 dataset has been dowloaded.')\n",
        "  \n",
        "\n",
        "# @markdown Load MNIST and CIFAR10 image datasets\n",
        "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
        "\n",
        "# MNIST\n",
        "mnist = datasets.MNIST('./mnist/',\n",
        "                       train=True,\n",
        "                       transform=transforms.ToTensor(),\n",
        "                       download=False)\n",
        "mnist_val = datasets.MNIST('./mnist/',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=False)\n",
        "\n",
        "# CIFAR 10\n",
        "cifar10 = datasets.CIFAR10('./cifar10/',\n",
        "                           train=True,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=False)\n",
        "cifar10_val = datasets.CIFAR10('./cifar10/',\n",
        "                               train=False,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=False)\n",
        "\n",
        "def get_data(name='mnist'):\n",
        "  if name == 'mnist':\n",
        "    my_dataset_name = \"MNIST\"\n",
        "    my_dataset = mnist\n",
        "    my_valset = mnist_val\n",
        "    my_dataset_shape = (1, 28, 28)\n",
        "    my_dataset_size = 28 * 28\n",
        "  elif name == 'cifar10':\n",
        "    my_dataset_name = \"CIFAR10\"\n",
        "    my_dataset = cifar10\n",
        "    my_valset = cifar10_val\n",
        "    my_dataset_shape = (3, 32, 32)\n",
        "    my_dataset_size = 3 * 32 * 32\n",
        "\n",
        "  return my_dataset, my_dataset_name, my_dataset_shape, my_dataset_size, my_valset\n",
        "\n",
        "\n",
        "train_set, dataset_name, data_shape, data_size, valid_set = get_data(name='mnist')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST dataset has been dowloaded.\n",
            "\n",
            "CIFAR10 dataset has been dowloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqJGvpAf3zhJ"
      },
      "source": [
        "class BiasLayer(nn.Module):\n",
        "  def __init__(self, shape):\n",
        "    super(BiasLayer, self).__init__()\n",
        "    init_bias = torch.zeros(shape)\n",
        "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.bias\n",
        "\n",
        "\n",
        "def cout(x, layer):\n",
        "  \"\"\"Unnecessarily complicated but complete way to\n",
        "  calculate the output depth, height and width size for a Conv2D layer\n",
        "\n",
        "  Args:\n",
        "    x (tuple): input size (depth, height, width)\n",
        "    layer (nn.Conv2d): the Conv2D layer\n",
        "\n",
        "  returns:\n",
        "    (int): output shape as given in [Ref]\n",
        "\n",
        "  Ref:\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "  \"\"\"\n",
        "  assert isinstance(layer, nn.Conv2d)\n",
        "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
        "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
        "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
        "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
        "  in_depth, in_height, in_width = x\n",
        "  out_depth = layer.out_channels\n",
        "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
        "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
        "  return (out_depth, out_height, out_width)\n",
        "\n",
        "\n",
        "# @title Helper functions\n",
        "\n",
        "#@title Helper functions\n",
        "\n",
        "def image_moments(image_batches, n_batches=None):\n",
        "  \"\"\"\n",
        "  Compute mean an covariance of all pixels from batches of images\n",
        "  \"\"\"\n",
        "  m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n",
        "  n = 0\n",
        "  for im in tqdm(image_batches, total=n_batches, leave=False,\n",
        "                 desc='Computing pixel mean and covariance...'):\n",
        "    im = im.to(DEVICE)\n",
        "    b = im.size()[0]\n",
        "    im = im.view(b, -1)\n",
        "    m1 = m1 + im.sum(dim=0)\n",
        "    m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n",
        "    n += b\n",
        "  m1, m2 = m1/n, m2/n\n",
        "  cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n",
        "  return m1.cpu(), cov.cpu()\n",
        "\n",
        "\n",
        "def interpolate(A, B, num_interps):\n",
        "  if A.shape != B.shape:\n",
        "    raise ValueError('A and B must have the same shape to interpolate.')\n",
        "  alphas = np.linspace(0, 1, num_interps)\n",
        "  return np.array([(1-a)*A + a*B for a in alphas])\n",
        "\n",
        "\n",
        "def kl_q_p(zs, phi):\n",
        "  \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
        "  phi must be size [b,k+1]\n",
        "\n",
        "  This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
        "  just -1/2*(zs**2)\n",
        "  \"\"\"\n",
        "  b, n, k = zs.size()\n",
        "  mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
        "  log_p = -0.5*(zs**2)\n",
        "  log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
        "  # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
        "  return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def log_p_x(x, mu_xs, sig_x):\n",
        "  \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
        "  pixel-wise log Gaussian probability\n",
        "\n",
        "  Sum over pixel dimensions, but mean over batch and samples.\n",
        "  \"\"\"\n",
        "  b, n = mu_xs.size()[:2]\n",
        "  # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
        "  # implicitly expanded when combined with mu_xs\n",
        "  x = x.reshape(b, 1, -1)\n",
        "  _, _, p = x.size()\n",
        "  squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
        "\n",
        "  # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
        "  # Expected value requires mean over [n]. Handling different size batches\n",
        "  # requires mean over [b].\n",
        "  return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
        "\n",
        "\n",
        "def pca_encoder_decoder(mu, cov, k):\n",
        "  \"\"\"\n",
        "  Compute encoder and decoder matrices for PCA dimensionality reduction\n",
        "  \"\"\"\n",
        "  mu = mu.view(1,-1)\n",
        "  u, s, v = torch.svd_lowrank(cov, q=k)\n",
        "  W_encode = v / torch.sqrt(s)\n",
        "  W_decode = u * torch.sqrt(s)\n",
        "\n",
        "  def pca_encode(x):\n",
        "    # Encoder: subtract mean image and project onto top K eigenvectors of\n",
        "    # the data covariance\n",
        "    return (x.view(-1,mu.numel()) - mu) @ W_encode\n",
        "\n",
        "  def pca_decode(h):\n",
        "    # Decoder: un-project then add back in the mean\n",
        "    return (h @ W_decode.T) + mu\n",
        "\n",
        "  return pca_encode, pca_decode\n",
        "\n",
        "\n",
        "def cout(x, layer):\n",
        "  \"\"\"Unnecessarily complicated but complete way to\n",
        "  calculate the output depth, height and width size for a Conv2D layer\n",
        "\n",
        "  Args:\n",
        "    x (tuple): input size (depth, height, width)\n",
        "    layer (nn.Conv2d): the Conv2D layer\n",
        "\n",
        "  returns:\n",
        "    (int): output shape as given in [Ref]\n",
        "\n",
        "  Ref:\n",
        "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "  \"\"\"\n",
        "  assert isinstance(layer, nn.Conv2d)\n",
        "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
        "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
        "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
        "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
        "  in_depth, in_height, in_width = x\n",
        "  out_depth = layer.out_channels\n",
        "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
        "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
        "  return (out_depth, out_height, out_width)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWoN9EZf9j9Q"
      },
      "source": [
        "Convolutional Auto Encoder [FULL]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F929qabx7PXM"
      },
      "source": [
        "K_VAE = 1024\n",
        "\n",
        "class ConvVarAutoEncoder(nn.Module):\n",
        "  def __init__(self, K, num_filters=[192, 256, 384, 512, 768], filter_size=3):\n",
        "    super(ConvVarAutoEncoder, self).__init__()\n",
        "    ## 5 Conv Layers\n",
        "    filter_reduction = 5 * (filter_size // 2)\n",
        "    self.fs = 0\n",
        "\n",
        "    self.shape_after_conv = (768,\n",
        "                             data_shape[1] - 2 * filter_reduction,\n",
        "                             data_shape[2] - 2 * filter_reduction)\n",
        "    \n",
        "    # Double for each additional layer of Conv\n",
        "    flat_size_after_conv = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
        "\n",
        "    # ENCODER\n",
        "    self.q_bias = BiasLayer(data_shape)\n",
        "    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters[0], filter_size)\n",
        "    self.q_conv_2 = nn.Conv2d(num_filters[0], num_filters[1], filter_size)\n",
        "    self.q_conv_3 = nn.Conv2d(num_filters[1], num_filters[2], filter_size)\n",
        "    self.q_conv_4 = nn.Conv2d(num_filters[2], num_filters[3], filter_size)\n",
        "    self.q_conv_5 = nn.Conv2d(num_filters[3], num_filters[4], filter_size)\n",
        "    self.q_flatten = nn.Flatten()\n",
        "    self.q_fc_phi = nn.Linear(248832, K+1)\n",
        "\n",
        "    # DECODER\n",
        "    self.p_fc_upsample = nn.Linear(K, 248832)\n",
        "    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
        "    self.p_deconv_1 = nn.ConvTranspose2d(num_filters[4], num_filters[3], filter_size)\n",
        "    self.p_deconv_2 = nn.ConvTranspose2d(num_filters[3], num_filters[2], filter_size)\n",
        "    self.p_deconv_3 = nn.ConvTranspose2d(num_filters[2], num_filters[1], filter_size)\n",
        "    self.p_deconv_4 = nn.ConvTranspose2d(num_filters[1], num_filters[0], filter_size)\n",
        "    self.p_deconv_5 = nn.ConvTranspose2d(num_filters[0], data_shape[0], filter_size)\n",
        "\n",
        "    self.p_bias = BiasLayer(data_shape)\n",
        "\n",
        "    # Define a special extra parameter to learn scalar sig_x for all pixels\n",
        "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
        "\n",
        "\n",
        "  def infer(self, x):\n",
        "    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
        "    rsample to get z\n",
        "    \"\"\"\n",
        "    s = self.q_bias(x)\n",
        "    s = F.relu(self.q_conv_1(s))\n",
        "    s = F.\n",
        "    s = F.relu(self.q_conv_2(s))\n",
        "    s = F.relu(self.q_conv_3(s))\n",
        "    s = F.relu(self.q_conv_4(s))\n",
        "    s = F.relu(self.q_conv_5(s))\n",
        "    flat_s = s.view(s.size()[0], -1)\n",
        "    self.fs = s.size()\n",
        "    print(\"FLAT S\")\n",
        "    print(s.size())\n",
        "    phi = self.q_fc_phi(flat_s)\n",
        "    return phi\n",
        "\n",
        "\n",
        "  def generate(self, zs):\n",
        "    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
        "    \"\"\"\n",
        "    # Note that for the purposes of passing through the generator, we need\n",
        "    # to reshape zs to be size [b*n,k]\n",
        "    b, n, k = zs.size()\n",
        "    s = zs.view(b*n, -1)\n",
        "    s = F.relu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
        "    s = F.relu(self.p_deconv_1(s))\n",
        "    s = F.relu(self.p_deconv_2(s))\n",
        "    s = F.relu(self.p_deconv_3(s))\n",
        "    s = F.relu(self.p_deconv_4(s))\n",
        "    s = self.p_deconv_5(s)\n",
        "    s = self.p_bias(s)\n",
        "    mu_xs = s.view(b, n, -1)\n",
        "    return mu_xs\n",
        "\n",
        "\n",
        "  def decode(self, zs):\n",
        "    # Included for compatability with conv-AE code\n",
        "    return self.generate(zs.unsqueeze(0))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # VAE.forward() is not used for training, but we'll treat it like a\n",
        "    # classic autoencoder by taking a single sample of z ~ q\n",
        "    phi = self.infer(x)\n",
        "    zs = rsample(phi, 1)\n",
        "    return self.generate(zs).view(x.size())\n",
        "\n",
        "\n",
        "  def elbo(self, x, n=1):\n",
        "    \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
        "    samples of z\n",
        "    \"\"\"\n",
        "    phi = self.infer(x)\n",
        "    zs = rsample(phi, n)\n",
        "    mu_xs = self.generate(zs)\n",
        "    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
        "\n",
        "\n",
        "def expected_z(phi):\n",
        "  return phi[:, :-1]\n",
        "\n",
        "\n",
        "def rsample(phi, n_samples):\n",
        "  \"\"\"Sample z ~ q(z;phi)\n",
        "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
        "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
        "  standard deviation\n",
        "  \"\"\"\n",
        "  b, kplus1 = phi.size()\n",
        "  k = kplus1-1\n",
        "  mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
        "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
        "  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n",
        "\n",
        "\n",
        "def train_vae(vae, dataset, epochs=10, n_samples=1000):\n",
        "  opt = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0)\n",
        "  elbo_vals = []\n",
        "  vae.to(DEVICE)\n",
        "  vae.train()\n",
        "  loader = DataLoader(dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
        "  for epoch in trange(epochs, desc='Epochs'):\n",
        "    for im, _ in tqdm(loader, total=len(dataset) // 10, desc='Batches', leave=False):\n",
        "      im = im.to(DEVICE)\n",
        "      opt.zero_grad()\n",
        "      loss = -vae.elbo(im)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      elbo_vals.append(-loss.item())\n",
        "  vae.to('cpu')\n",
        "  vae.eval()\n",
        "  return elbo_vals\n",
        "\n",
        "\n",
        "convVAE = ConvVarAutoEncoder(K=K_VAE)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfCmpcBv8Dlt",
        "outputId": "bb6662ff-5bba-4baa-9819-4f70391bc9ef"
      },
      "source": [
        "convVAE"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvVarAutoEncoder(\n",
              "  (q_bias): BiasLayer()\n",
              "  (q_conv_1): Conv2d(1, 192, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_2): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_4): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_conv_5): Conv2d(512, 768, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (q_flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (q_fc_phi): Linear(in_features=248832, out_features=1025, bias=True)\n",
              "  (p_fc_upsample): Linear(in_features=1024, out_features=248832, bias=True)\n",
              "  (p_unflatten): Unflatten(dim=-1, unflattened_size=(768, 18, 18))\n",
              "  (p_deconv_1): ConvTranspose2d(768, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_2): ConvTranspose2d(512, 384, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_3): ConvTranspose2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_4): ConvTranspose2d(256, 192, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_deconv_5): ConvTranspose2d(192, 1, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (p_bias): BiasLayer()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac8t_FT48Und"
      },
      "source": [
        "DEVICE = 'cpu'\n",
        "\n",
        "#trained_CVAE = train_vae(convVAE, train_set, epochs = 1, n_samples = 10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liha1iaE4NsC",
        "outputId": "ab120f13-f97b-4c1e-b483-6b932639b172"
      },
      "source": [
        "p = 0\n",
        "for param in convVAE.parameters():\n",
        "  p += torch.numel(param)\n",
        "\n",
        "p"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "523386147"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aRD2Cx4XsY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}