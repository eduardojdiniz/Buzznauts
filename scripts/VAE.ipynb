{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a   href=\"https://colab.research.google.com/github/eduardojdiniz/Buzznauts/blob/master/scripts/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duecredit --quiet\n",
    "!git clone https://github.com/eduardojdiniz/Buzznauts --quiet\n",
    "!pip install torchinfo --quiet\n",
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
    "# Imports\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "# Initialize instance of W&B\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download MNIST and CIFAR10 datasets\n",
    "import tarfile, requests, os\n",
    "\n",
    "fname = 'MNIST.tar.gz'\n",
    "name = 'mnist'\n",
    "url = 'https://osf.io/y2fj6/download'\n",
    "\n",
    "if not os.path.exists(name):\n",
    "  print('\\nDownloading MNIST dataset...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fh:\n",
    "    fh.write(r.content)\n",
    "  print('\\nDownloading MNIST completed..\\n')\n",
    "\n",
    "if not os.path.exists(name):\n",
    "  with tarfile.open(fname) as tar:\n",
    "    tar.extractall(name)\n",
    "    os.remove(fname)\n",
    "else:\n",
    "  print('MNIST dataset has been dowloaded.\\n')\n",
    "\n",
    "\n",
    "fname = 'cifar-10-python.tar.gz'\n",
    "name = 'cifar10'\n",
    "url = 'https://osf.io/jbpme/download'\n",
    "\n",
    "if not os.path.exists(name):\n",
    "  print('\\nDownloading CIFAR10 dataset...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fh:\n",
    "    fh.write(r.content)\n",
    "  print('\\nDownloading CIFAR10 completed.')\n",
    "\n",
    "if not os.path.exists(name):\n",
    "  with tarfile.open(fname) as tar:\n",
    "    tar.extractall(name)\n",
    "    os.remove(fname)\n",
    "else:\n",
    "  print('CIFAR10 dataset has been dowloaded.')\n",
    "  \n",
    "\n",
    "# @markdown Load MNIST and CIFAR10 image datasets\n",
    "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "# MNIST\n",
    "mnist = datasets.MNIST('./mnist/',\n",
    "                       train=True,\n",
    "                       transform=transforms.ToTensor(),\n",
    "                       download=False)\n",
    "mnist_val = datasets.MNIST('./mnist/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=False)\n",
    "\n",
    "# CIFAR 10\n",
    "cifar10 = datasets.CIFAR10('./cifar10/',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=False)\n",
    "cifar10_val = datasets.CIFAR10('./cifar10/',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=False)\n",
    "\n",
    "def get_data(name='mnist'):\n",
    "  if name == 'mnist':\n",
    "    my_dataset_name = \"MNIST\"\n",
    "    my_dataset = mnist\n",
    "    my_valset = mnist_val\n",
    "    my_dataset_shape = (1, 28, 28)\n",
    "    my_dataset_size = 28 * 28\n",
    "  elif name == 'cifar10':\n",
    "    my_dataset_name = \"CIFAR10\"\n",
    "    my_dataset = cifar10\n",
    "    my_valset = cifar10_val\n",
    "    my_dataset_shape = (3, 32, 32)\n",
    "    my_dataset_size = 3 * 32 * 32\n",
    "\n",
    "  return my_dataset, my_dataset_name, my_dataset_shape, my_dataset_size, my_valset\n",
    "\n",
    "\n",
    "train_set, dataset_name, data_shape, data_size, valid_set = get_data(name='cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "  def __init__(self, shape):\n",
    "    super(BiasLayer, self).__init__()\n",
    "    init_bias = torch.zeros(shape)\n",
    "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.bias\n",
    "\n",
    "\n",
    "def cout(x, layer):\n",
    "  \"\"\"Unnecessarily complicated but complete way to\n",
    "  calculate the output depth, height and width size for a Conv2D layer\n",
    "\n",
    "  Args:\n",
    "    x (tuple): input size (depth, height, width)\n",
    "    layer (nn.Conv2d): the Conv2D layer\n",
    "\n",
    "  returns:\n",
    "    (int): output shape as given in [Ref]\n",
    "\n",
    "  Ref:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "  \"\"\"\n",
    "  assert isinstance(layer, nn.Conv2d)\n",
    "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
    "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
    "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
    "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
    "  in_depth, in_height, in_width = x\n",
    "  out_depth = layer.out_channels\n",
    "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
    "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
    "  return (out_depth, out_height, out_width)\n",
    "\n",
    "\n",
    "# @title Helper functions\n",
    "\n",
    "#@title Helper functions\n",
    "\n",
    "def image_moments(image_batches, n_batches=None):\n",
    "  \"\"\"\n",
    "  Compute mean an covariance of all pixels from batches of images\n",
    "  \"\"\"\n",
    "  m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n",
    "  n = 0\n",
    "  for im in tqdm(image_batches, total=n_batches, leave=False,\n",
    "                 desc='Computing pixel mean and covariance...'):\n",
    "    im = im.to(DEVICE)\n",
    "    b = im.size()[0]\n",
    "    im = im.view(b, -1)\n",
    "    m1 = m1 + im.sum(dim=0)\n",
    "    m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n",
    "    n += b\n",
    "  m1, m2 = m1/n, m2/n\n",
    "  cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n",
    "  return m1.cpu(), cov.cpu()\n",
    "\n",
    "\n",
    "def interpolate(A, B, num_interps):\n",
    "  if A.shape != B.shape:\n",
    "    raise ValueError('A and B must have the same shape to interpolate.')\n",
    "  alphas = np.linspace(0, 1, num_interps)\n",
    "  return np.array([(1-a)*A + a*B for a in alphas])\n",
    "\n",
    "\n",
    "def kl_q_p(zs, phi):\n",
    "  \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n",
    "  phi must be size [b,k+1]\n",
    "\n",
    "  This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n",
    "  just -1/2*(zs**2)\n",
    "  \"\"\"\n",
    "  b, n, k = zs.size()\n",
    "  mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n",
    "  log_p = -0.5*(zs**2)\n",
    "  log_q = -0.5*(zs - mu_q.view(b,1,k))**2 / log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n",
    "  # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n",
    "  return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n",
    "\n",
    "\n",
    "def log_p_x(x, mu_xs, sig_x):\n",
    "  \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n",
    "  pixel-wise log Gaussian probability\n",
    "\n",
    "  Sum over pixel dimensions, but mean over batch and samples.\n",
    "  \"\"\"\n",
    "  b, n = mu_xs.size()[:2]\n",
    "  # Flatten out pixels and add a singleton dimension [1] so that x will be\n",
    "  # implicitly expanded when combined with mu_xs\n",
    "  x = x.reshape(b, 1, -1)\n",
    "  _, _, p = x.size()\n",
    "  squared_error = (x - mu_xs.view(b, n, -1))**2 / (2*sig_x**2)\n",
    "\n",
    "  # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n",
    "  # Expected value requires mean over [n]. Handling different size batches\n",
    "  # requires mean over [b].\n",
    "  return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n",
    "\n",
    "\n",
    "def pca_encoder_decoder(mu, cov, k):\n",
    "  \"\"\"\n",
    "  Compute encoder and decoder matrices for PCA dimensionality reduction\n",
    "  \"\"\"\n",
    "  mu = mu.view(1,-1)\n",
    "  u, s, v = torch.svd_lowrank(cov, q=k)\n",
    "  W_encode = v / torch.sqrt(s)\n",
    "  W_decode = u * torch.sqrt(s)\n",
    "\n",
    "  def pca_encode(x):\n",
    "    # Encoder: subtract mean image and project onto top K eigenvectors of\n",
    "    # the data covariance\n",
    "    return (x.view(-1,mu.numel()) - mu) @ W_encode\n",
    "\n",
    "  def pca_decode(h):\n",
    "    # Decoder: un-project then add back in the mean\n",
    "    return (h @ W_decode.T) + mu\n",
    "\n",
    "  return pca_encode, pca_decode\n",
    "\n",
    "\n",
    "def cout(x, layer):\n",
    "  \"\"\"Unnecessarily complicated but complete way to\n",
    "  calculate the output depth, height and width size for a Conv2D layer\n",
    "\n",
    "  Args:\n",
    "    x (tuple): input size (depth, height, width)\n",
    "    layer (nn.Conv2d): the Conv2D layer\n",
    "\n",
    "  returns:\n",
    "    (int): output shape as given in [Ref]\n",
    "\n",
    "  Ref:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "  \"\"\"\n",
    "  assert isinstance(layer, nn.Conv2d)\n",
    "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
    "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
    "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
    "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
    "  in_depth, in_height, in_width = x\n",
    "  out_depth = layer.out_channels\n",
    "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
    "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
    "  return (out_depth, out_height, out_width)\n",
    "\n",
    "\n",
    "def load_vaegan_weights(model, pretrained_path):\n",
    "  # load pretrained weights\n",
    "  pretrained_fn = open(pretrained_path,'rb')\n",
    "  pretrained = pickle.load(pretrained_fn)\n",
    "\n",
    "  # have a look what's in the pretrained file\n",
    "  old_keynames=[]\n",
    "  for key, value in pretrained.items() :\n",
    "    print (key,value.shape) \n",
    "    old_keynames.append(key) \n",
    "\n",
    "  # get the keynames of our model\n",
    "  curr_state=model.state_dict()\n",
    "  new_keynames=[]\n",
    "  for key, value in curr_state.items() :\n",
    "      if key.startswith('q_conv'):\n",
    "        new_keynames.append(key)\n",
    "\n",
    "  # change the names of the pretrained model to match our model\n",
    "  for i in range(len(old_keynames)):\n",
    "    pretrained[new_keynames[i]] = pretrained[old_keynames[i]]\n",
    "    del pretrained[old_keynames[i]]\n",
    "\n",
    "  # change size & make the weights a torch\n",
    "  # In TF, Conv2d filter shape is [filter_height, filter_width, in_channels, out_channels],\n",
    "  # while in Pytorch is (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "  # SO we need to permute [3,2,0,1]\n",
    "  for key, value in pretrained.items() :\n",
    "    if len(value.shape)==4:\n",
    "      new_val=torch.tensor(value)\n",
    "      new_val=new_val.permute(3,2,0,1)\n",
    "    else: \n",
    "      new_val=torch.tensor(value)\n",
    "    \n",
    "    pretrained[key] = new_val\n",
    "\n",
    "  return pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Auto Encoder [FULL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VAE = 1024\n",
    "\n",
    "class ConvVarAutoEncoder(nn.Module):\n",
    "  def __init__(self, K, num_filters=[192, 256, 384, 512, 768], filter_size=3):\n",
    "    super(ConvVarAutoEncoder, self).__init__()\n",
    "    ## 5 Conv Layers\n",
    "    filter_reduction = 5 * (filter_size // 2)\n",
    "\n",
    "    self.shape_after_conv = calc_output_size(data_shape, filter_size, num_filters)\n",
    "    \n",
    "    self.flat_shape = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
    "    \n",
    "    # Double for each additional layer of Conv\n",
    "    flat_size_after_conv = self.shape_after_conv[0] * self.shape_after_conv[1] * self.shape_after_conv[2]\n",
    "\n",
    "    # ENCODER\n",
    "    self.q_bias = BiasLayer(data_shape)\n",
    "    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters[0], filter_size)\n",
    "    self.q_conv_2 = nn.Conv2d(num_filters[0], num_filters[1], filter_size)\n",
    "    self.q_conv_3 = nn.Conv2d(num_filters[1], num_filters[2], filter_size)\n",
    "    self.q_conv_4 = nn.Conv2d(num_filters[2], num_filters[3], filter_size)\n",
    "    self.q_conv_5 = nn.Conv2d(num_filters[3], num_filters[4], filter_size)\n",
    "    self.q_flatten = nn.Flatten()\n",
    "    self.q_fc_phi = nn.Linear(self.flat_shape, K+1)\n",
    "\n",
    "    # DECODER\n",
    "    self.p_fc_upsample = nn.Linear(K, self.flat_shape)\n",
    "    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n",
    "    self.p_deconv_1 = nn.ConvTranspose2d(num_filters[4], num_filters[3], filter_size)\n",
    "    self.p_deconv_2 = nn.ConvTranspose2d(num_filters[3], num_filters[2], filter_size)\n",
    "    self.p_deconv_3 = nn.ConvTranspose2d(num_filters[2], num_filters[1], filter_size)\n",
    "    self.p_deconv_4 = nn.ConvTranspose2d(num_filters[1], num_filters[0], filter_size)\n",
    "    self.p_deconv_5 = nn.ConvTranspose2d(num_filters[0], data_shape[0], filter_size)\n",
    "\n",
    "    self.p_bias = BiasLayer(data_shape)\n",
    "\n",
    "    # Define a special extra parameter to learn scalar sig_x for all pixels\n",
    "    self.log_sig_x = nn.Parameter(torch.zeros(()))\n",
    "\n",
    "\n",
    "  def infer(self, x):\n",
    "    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n",
    "    rsample to get z\n",
    "    \"\"\"\n",
    "    s = self.q_bias(x)\n",
    "    s = F.elu(self.q_conv_1(s))\n",
    "    s = F.elu(self.q_conv_2(s))\n",
    "    s = F.elu(self.q_conv_3(s))\n",
    "    s = F.elu(self.q_conv_4(s))\n",
    "    s = F.elu(self.q_conv_5(s))\n",
    "    flat_s = s.view(s.size()[0], -1)\n",
    "    phi = self.q_fc_phi(flat_s)\n",
    "    return phi\n",
    "\n",
    "\n",
    "  def generate(self, zs):\n",
    "    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n",
    "    \"\"\"\n",
    "    # Note that for the purposes of passing through the generator, we need\n",
    "    # to reshape zs to be size [b*n,k]\n",
    "    b, n, k = zs.size()\n",
    "    s = zs.view(b*n, -1)\n",
    "    s = F.elu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n",
    "    s = F.elu(self.p_deconv_1(s))\n",
    "    s = F.elu(self.p_deconv_2(s))\n",
    "    s = F.elu(self.p_deconv_3(s))\n",
    "    s = F.elu(self.p_deconv_4(s))\n",
    "    s = self.p_deconv_5(s)\n",
    "    s = self.p_bias(s)\n",
    "    mu_xs = s.view(b, n, -1)\n",
    "    return mu_xs\n",
    "\n",
    "\n",
    "  def decode(self, zs):\n",
    "    # Included for compatability with conv-AE code\n",
    "    return self.generate(zs.unsqueeze(0))\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # VAE.forward() is not used for training, but we'll treat it like a\n",
    "    # classic autoencoder by taking a single sample of z ~ q\n",
    "    phi = self.infer(x)\n",
    "    zs = rsample(phi, 1)\n",
    "    return self.generate(zs).view(x.size())\n",
    "\n",
    "\n",
    "  def elbo(self, x, n=1):\n",
    "    \"\"\"Run input end to end through the VAE and compute the ELBO using n\n",
    "    samples of z\n",
    "    \"\"\"\n",
    "    phi = self.infer(x)\n",
    "    zs = rsample(phi, n)\n",
    "    mu_xs = self.generate(zs)\n",
    "    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi)\n",
    "\n",
    "  \n",
    "  def load_my_state_dict(self, state_dict):\n",
    "    curr_state=self.state_dict()\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "      print(type(param))\n",
    "      if name not in curr_state:\n",
    "        print('name does not exist -- skipping')\n",
    "        continue\n",
    "      if isinstance(param, torch.Tensor):\n",
    "          param = param.data\n",
    "      curr_state[name].copy_(param)\n",
    "\n",
    "\n",
    "###============================FUNCTIONS=========================###\n",
    "\n",
    "\n",
    "def expected_z(phi):\n",
    "  return phi[:, :-1]\n",
    "\n",
    "\n",
    "def rsample(phi, n_samples):\n",
    "  \"\"\"Sample z ~ q(z;phi)\n",
    "  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n",
    "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "  standard deviation\n",
    "  \"\"\"\n",
    "  b, kplus1 = phi.size()\n",
    "  k = kplus1-1\n",
    "  mu, sig = phi[:, :-1], phi[:,-1].exp()\n",
    "  eps = torch.randn(b, n_samples, k, device=phi.device)\n",
    "  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n",
    "\n",
    "\n",
    "def train_vae(vae, dataset, epochs=10, n_samples=1000, freeze_idx = []):\n",
    "\n",
    "  for idx, param in enumerate(vae.parameters()): \n",
    "    if idx in freeze_idx: param.requires_grad = False\n",
    "\n",
    "  print(\"===Freezing layers:===\")\n",
    "\n",
    "  for name, param in vae.named_parameters(): \n",
    "    if param.requires_grad == False: print(\"\\t\" + name)\n",
    "\n",
    "  \n",
    "\n",
    "  # passing only those parameters that explicitly requires grad\n",
    "  opt = torch.optim.Adam(filter(lambda p: p.requires_grad, vae.parameters()), lr=1e-3, weight_decay=0)\n",
    "  elbo_vals = []\n",
    "  vae.to(DEVICE)\n",
    "  vae.train()\n",
    "  loader = DataLoader(dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "  for epoch in trange(epochs, desc='Epochs'):\n",
    "    for im, _ in tqdm(loader, total=len(dataset) // 16, desc='Batches', leave=False):\n",
    "      im = im.to(DEVICE)\n",
    "      opt.zero_grad()\n",
    "      loss = -vae.elbo(im)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      elbo_vals.append(-loss.item())\n",
    "    \n",
    "      # W&B Logging at the end of each epoch\n",
    "      print(loss)\n",
    "      wandb.log(loss)\n",
    "\n",
    "  vae.to('cuda')\n",
    "  vae.eval()\n",
    "  return elbo_vals\n",
    "\n",
    "def calc_output_size(input_size, kernel_size, kchannels, padding=0, stride=1):\n",
    "  output_size = input_size\n",
    "  for kc in kchannels:\n",
    "    output_height = (output_size[1] + padding + padding - kernel_size) / (stride) + 1\n",
    "    output_width = (output_size[2] + padding + padding - kernel_size) / (stride) + 1\n",
    "\n",
    "    output_size = [kc, int(output_height), int(output_width)]\n",
    "    print(output_size)\n",
    "\n",
    "  return tuple(output_size)\n",
    "\n",
    "                     \n",
    "\n",
    "\n",
    "\n",
    "convVAE = ConvVarAutoEncoder(K=K_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = '/content/drive/MyDrive/Buzznauts/data/pretrained/vaegan_enc_weights.pickle'\n",
    "pretrained = load_vaegan_weights(convVAE, pretrained_path)\n",
    "convVAE.load_my_state_dict(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "wandb.watch(convVAE)\n",
    "trained_CVAE = train_vae(convVAE, train_set, epochs = 1, n_samples = 10, freeze_idx = [2, 3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05251c6ee2824476825857669474bbd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b12c4866024855b96441d9af95158b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23681221c03c41eeb621a56f7b7ef037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d2f4ec50ef4091a5ca222a61c1ce48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36e95b8e5c0f4fc49eebe988cdc69657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45110124517b4a849eecb54ba214483b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b4ff4a735d74741a16c28631eeb0e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b7665c14274ce5820be74acc04b48c",
      "placeholder": "​",
      "style": "IPY_MODEL_a789ff430c704c88b424e3c703837826",
      "value": " 0/1 [00:00&lt;?, ?it/s]"
     }
    },
    "61f7542ccddb437786f81f8e2eaf64df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75beb26e5a90448884c99b52c13e403e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23681221c03c41eeb621a56f7b7ef037",
      "placeholder": "​",
      "style": "IPY_MODEL_36e95b8e5c0f4fc49eebe988cdc69657",
      "value": "Epochs:   0%"
     }
    },
    "859e4ea521ab43f6b114cc85c92ce364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93ec85a8904f4624a68cabc1f6f4c7ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44fa98329b9437f81289703de125680",
      "placeholder": "​",
      "style": "IPY_MODEL_859e4ea521ab43f6b114cc85c92ce364",
      "value": " 0/3125 [00:00&lt;?, ?it/s]"
     }
    },
    "9c8f9ce945f142a4b6c73bbf47331bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61f7542ccddb437786f81f8e2eaf64df",
      "placeholder": "​",
      "style": "IPY_MODEL_d03ad9486fae4c4b8710f8f72406fc6a",
      "value": "Batches:   0%"
     }
    },
    "a789ff430c704c88b424e3c703837826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0fc935f5caf486aafec9289828f0342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75beb26e5a90448884c99b52c13e403e",
       "IPY_MODEL_db2e31b42ee84fc68b70c48dd1804412",
       "IPY_MODEL_5b4ff4a735d74741a16c28631eeb0e89"
      ],
      "layout": "IPY_MODEL_24d2f4ec50ef4091a5ca222a61c1ce48"
     }
    },
    "b60b63f8845d40b983df3bbecda64574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce6482f2519f41a5897da75db74951cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05251c6ee2824476825857669474bbd9",
      "max": 3125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b60b63f8845d40b983df3bbecda64574",
      "value": 0
     }
    },
    "d03ad9486fae4c4b8710f8f72406fc6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0b7665c14274ce5820be74acc04b48c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d44fa98329b9437f81289703de125680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db2e31b42ee84fc68b70c48dd1804412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5e38eac46734a63a4b8bbc1e232e2dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45110124517b4a849eecb54ba214483b",
      "value": 0
     }
    },
    "f3e5444497f04c3fb0448a2c9beca7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c8f9ce945f142a4b6c73bbf47331bcc",
       "IPY_MODEL_ce6482f2519f41a5897da75db74951cf",
       "IPY_MODEL_93ec85a8904f4624a68cabc1f6f4c7ad"
      ],
      "layout": "IPY_MODEL_20b12c4866024855b96441d9af95158b"
     }
    },
    "f5e38eac46734a63a4b8bbc1e232e2dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
